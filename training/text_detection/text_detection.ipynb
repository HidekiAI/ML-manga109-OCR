{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/HidekiAI/ML-manga109-OCR/blob/trunk/Untitled0.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "First, we want to make sure TensorFlow is installed in the Python (virtual) environment...\n",
        "\n",
        "-   TensorFlow Object Detection is now depracated\n",
        "-   TensorFlow Addons (for using TF-Vision) sunsets on May, 2024 and needs to be switched over to Keras, in which it should be accessible directly as long as TF is installed\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "S5BXth-cnCwW"
      },
      "outputs": [],
      "source": [
        "#!/bin/bash\n",
        "# NOTE: NO NEED to run this on CoLab, only on local...\n",
        "!pip install --upgrade pip\n",
        "\n",
        "!pip install -U --pre tensorflow==\"2.*\"\n",
        "!pip install tensorflow\n",
        "# Comment above and uncomment below if you want to install tensorflow-gpu instead of tensorflow on CoLab\n",
        "#!pip install tensorflow-gpu\n",
        "#pip install tensorflow[and-cuda]\n",
        "\n",
        "!pip install transformers\n",
        "!pip install tf-models-official\n",
        "!pip install tf-keras-vis"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Next, we'll need the (official) tools/libraries to read manga109 (annotation) data from https://github.com/manga109\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#!/bin/bash\n",
        "# MUST run ths on BOTH CoLab and local...\n",
        "!pip install manga109api"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "I want to know which version of TF is installed, I cannot run GPU version on my local machine...\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#!/usr/bin/env python\n",
        "# Optionally run this to check the TensorFlow version and configuration\n",
        "\n",
        "import tensorflow as tf\n",
        "\n",
        "\n",
        "# Check TensorFlow version\n",
        "\n",
        "print(\"TensorFlow version:\", tf.__version__)\n",
        "\n",
        "\n",
        "# Check TensorFlow configuration\n",
        "\n",
        "print(\"TensorFlow configuration:\")\n",
        "\n",
        "print(tf.config.list_physical_devices('GPU'))  # List available GPUs\n",
        "\n",
        "print(tf.config.list_physical_devices('CPU'))  # List available CPUs"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Next, I'd like to absolutely make sure we have access to TF-Vision for text detection; Because tensorflow-addons has become sunset as of May, 2024, we just need to verify that keras is accessible...\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#!/usr/bin/env python\n",
        "# Optionally run this to check the TensorFlow version and configuration\n",
        "\n",
        "from tensorflow.keras.layers import Dense, Conv2D, Flatten, MaxPooling2D\n",
        "\n",
        "from tensorflow.keras.models import Sequential\n",
        "\n",
        "import tensorflow as tf\n",
        "\n",
        "\n",
        "# Check TensorFlow version\n",
        "\n",
        "print(\"TensorFlow version:\", tf.__version__)\n",
        "\n",
        "\n",
        "# Access Keras functionality through tf.keras\n",
        "\n",
        "\n",
        "# Define a simple Sequential model\n",
        "\n",
        "model = Sequential([\n",
        "\n",
        "    Conv2D(16, 3, padding='same', activation='relu', input_shape=(32, 32, 3)),\n",
        "\n",
        "    MaxPooling2D(),\n",
        "\n",
        "    Conv2D(32, 3, padding='same', activation='relu'),\n",
        "\n",
        "    MaxPooling2D(),\n",
        "\n",
        "    Conv2D(64, 3, padding='same', activation='relu'),\n",
        "\n",
        "    MaxPooling2D(),\n",
        "\n",
        "    Flatten(),\n",
        "\n",
        "    Dense(128, activation='relu'),\n",
        "\n",
        "    Dense(10, activation='softmax')\n",
        "\n",
        "])\n",
        "\n",
        "\n",
        "# Compile the model\n",
        "\n",
        "model.compile(optimizer='adam',\n",
        "\n",
        "              loss='sparse_categorical_crossentropy',\n",
        "\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "\n",
        "# Print model summary\n",
        "\n",
        "model.summary()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Once TF-Vision is loaded, let's verify for sure via Python...\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#!/usr/bin/env python\n",
        "# Optionally run this to check the TensorFlow version and configuration\n",
        "import tensorflow as tf\n",
        "\n",
        "from tensorflow.keras.applications import EfficientNetB0\n",
        "\n",
        "\n",
        "# Check TensorFlow version\n",
        "\n",
        "print(\"TensorFlow version:\", tf.__version__)\n",
        "\n",
        "\n",
        "# Try importing a TensorFlow Vision model (e.g., EfficientNet)\n",
        "\n",
        "try:\n",
        "\n",
        "    # Import the EfficientNetB0 model\n",
        "\n",
        "    model = EfficientNetB0(weights='imagenet')\n",
        "\n",
        "    print(\"TensorFlow Vision (via Keras) is accessible.\")\n",
        "\n",
        "except ImportError:\n",
        "\n",
        "    print(\"TensorFlow Vision (via Keras) is not accessible.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Note that below is ONLY necessary for Google CoLab to access your Google Drive. If on Notepad/Jupyter, do the following instead (not exact, just the example):\n",
        "\n",
        "-   Linux: make sure to `ln -sv ~/Google/MyDrive /content/drive` to softlink your Google G-Drive as `/content/drive`\n",
        "-   Windows: From DOS Command Prompt (right clock to launch as Admin) `mklink.exe /D \"C:/content/drive\" \"C:/Users/HidekiAI/Google/MyDrive/\"` to create a dir-junction\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#!/usr/bin/python\n",
        "# No need to execute this if running locally, this is only for Google CoLab usage\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Verify either via BASH or python that we can access `/content/drive` mount\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#!/bin/bash\n",
        "! pwd && [ -e /content/drive/MyDrive ] || echo \"Unable to validate Google Drive from bash script\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#!/usr/bin/env python\n",
        "import os\n",
        "\n",
        "# directory path to the Manga109 dataset (read-only)\n",
        "global manga109_dir\n",
        "# directory path to the TensorFlow TFRecord model (read-write)\n",
        "global tf_model_dir\n",
        "\n",
        "# Check if Google Drive is mounted and/or locally have symlink (or junctions) to access '/content/drive/MyDrive'\n",
        "if os.path.isdir('/content/drive'):\n",
        "    # list contents of the root directory of Google drive\n",
        "    # change this to your own path\n",
        "    root_paths = '/content/drive/MyDrive/projects/ML-manga-ocr-rust/'\n",
        "    data_paths = os.path.join(root_paths, 'data/')  # should pre-exist!\n",
        "    tf_model_dir = os.path.join(data_paths, 'tf_model/')\n",
        "    # mkdir if not exists\n",
        "    if not os.path.exists(tf_model_dir):\n",
        "        os.makedirs(tf_model_dir)\n",
        "        print('Created TensorFlow model directory at ', tf_model_dir)\n",
        "\n",
        "    drive_files = os.listdir(root_paths)\n",
        "    print(drive_files)\n",
        "    drive_files = os.listdir(data_paths)\n",
        "    print(drive_files)\n",
        "    zip_path = os.path.join(root_paths, 'data/Manga109s.zip')\n",
        "    if os.path.exists(zip_path):\n",
        "        # only UNZIP IF dir does not exist, else assume it's already unzipped\n",
        "        if not os.path.exists(data_paths):\n",
        "            # os.makedirs(data_paths)\n",
        "            #!unzip '{zip_path}' -d '{data_paths}'\n",
        "            with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
        "                zip_ref.extractall(data_paths)\n",
        "                print('Unzipped the data to ', data_paths)\n",
        "    drive_files = os.listdir(data_paths)\n",
        "    manga109_dir = os.path.join(\n",
        "        data_paths, 'Manga109s/Manga109s_released_2023_12_07/')\n",
        "    data_dir_files = os.listdir(manga109_dir)\n",
        "    print(data_dir_files)\n",
        "    # lastly, notify users of their license by printing the readme.txt\n",
        "    readme_path = os.path.join(manga109_dir, 'readme.txt')\n",
        "    with open(readme_path, 'r', encoding=\"utf-8\") as bookmark_file:\n",
        "        print(bookmark_file.read())\n",
        "else:\n",
        "    print(\"Google Drive is not mounted.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Now that we have manga dir accessible, let's try out the manga109api...\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#!/usr/bin/env python\n",
        "import manga109api\n",
        "from PIL import Image, ImageDraw\n",
        "\n",
        "\n",
        "def draw_rectangle(img, x0, y0, x1, y1, annotation_type):\n",
        "    assert annotation_type in [\"body\", \"face\", \"frame\", \"text\"]\n",
        "    color = {\"body\": \"#258039\", \"face\": \"#f5be41\",\n",
        "             \"frame\": \"#31a9b8\", \"text\": \"#cf3721\"}[annotation_type]\n",
        "    draw = ImageDraw.Draw(img)\n",
        "    draw.rectangle([x0, y0, x1, y1], outline=color, width=10)\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    book = \"YumeiroCooking\"\n",
        "    page_index = 6\n",
        "\n",
        "    p = manga109api.Parser(root_dir=manga109_dir)\n",
        "    annotation = p.get_annotation(book=book)\n",
        "    img = Image.open(p.img_path(book=book, index=page_index))\n",
        "\n",
        "    for annotation_type in [\"body\", \"face\", \"frame\", \"text\"]:\n",
        "        rois = annotation[\"page\"][page_index][annotation_type]\n",
        "        for roi in rois:\n",
        "            draw_rectangle(img, roi[\"@xmin\"], roi[\"@ymin\"],\n",
        "                           roi[\"@xmax\"], roi[\"@ymax\"], annotation_type)\n",
        "\n",
        "    # Display preprocessed image\n",
        "    import matplotlib.pyplot as plt\n",
        "    plt.imshow(img)\n",
        "    plt.axis('off')\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Load and Preprocess Images with TensorFlow:\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "If you did see an image load up with rectangles around texts, you are now ready to integrate it with TF-Vision...\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#!/usr/bin/env python\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import tensorflow as tf\n",
        "import manga109api\n",
        "from PIL import Image, ImageDraw\n",
        "\n",
        "# Initialize Manga109 API\n",
        "manga109 = manga109api.Parser(root_dir=manga109_dir)\n",
        "\n",
        "# Choose a manga volume and page index\n",
        "volume = 'YumeiroCooking'\n",
        "page_index = 6\n",
        "\n",
        "# Load image using Manga109 API\n",
        "image = Image.open(manga109.img_path(book=volume, index=page_index))\n",
        "\n",
        "# Preprocess image using TensorFlow Keras\n",
        "image = tf.keras.preprocessing.image.img_to_array(image)\n",
        "image = tf.keras.applications.efficientnet.preprocess_input(image)\n",
        "\n",
        "# Display preprocessed image\n",
        "plt.imshow(image)\n",
        "plt.axis('off')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "If the above worked for single book/volume, we can now iterate the ENTIRE books it knows about; There is a minor issue in which curated annotation file thinks there is a JPG associated to it, in which the images dir for that book no longer exists, so we'll have to do extra checks (extra I/O means performance) whether the file exists or not.\n",
        "We'll preprocess image prior to making it into TFRecord. Ideally, we'd want this to be on a separate cell, but it causes memory outage due to huge blocks of images, hence we'll check if image has text-regions, and if so, create a TFRecord for that region\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#!/usr/bin/env python\n",
        "import tensorflow as tf\n",
        "\n",
        "global tf_model_paths\n",
        "\n",
        "# Each text region looks like so:\n",
        "#       'text': [\n",
        "#           {'@id': '000000d2', '@xmin': 698, '@ymin': 238, '@xmax': 711, '@ymax': 284, '#text': 'ブン？', 'type': 'text'},\n",
        "#           {'@id': '000000dc', '@xmin': 356, '@ymin': 273, '@xmax': 403, '@ymax': 340, '#text': 'あの人.....', 'type': 'text'},\n",
        "#           {'@id': '000000de', '@xmin': 1131, '@ymin': 752, '@xmax': 1200, '@ymax': 841, '#text': 'わっ', 'type': 'text'},\n",
        "#           {'@id': '000000e0', '@xmin': 482, '@ymin': 91, '@xmax': 498, '@ymax': 145, '#text': 'あれ？', 'type': 'text'}]}\n",
        "# It seems that so far, best approach is to map a single (preprocessed) image to a set (list/array) of text regions\n",
        "def create_tf_manga109_rects_from_page(preprocessed_image, page_width, page_height, text_rects):\n",
        "    # Initialize lists for rectangle coordinates\n",
        "    xmin_list, ymin_list, xmax_list, ymax_list = [], [], [], []\n",
        "\n",
        "    for text_rect in text_rects:\n",
        "        # Append rectangle coordinates to the lists\n",
        "        xmin_list.append(text_rect['@xmin'])\n",
        "        ymin_list.append(text_rect['@ymin'])\n",
        "        xmax_list.append(text_rect['@xmax'])\n",
        "        ymax_list.append(text_rect['@ymax'])\n",
        "\n",
        "    # Create a TensorFlow Example from the image and the text regions\n",
        "    tf_example = tf.train.Example(features=tf.train.Features(feature={\n",
        "        'image': tf.train.Feature(bytes_list=tf.train.BytesList(value=[preprocessed_image.tobytes()])),\n",
        "        'page_width': tf.train.Feature(int64_list=tf.train.Int64List(value=[page_width])),\n",
        "        'page_height': tf.train.Feature(int64_list=tf.train.Int64List(value=[page_height])),\n",
        "        'xmin': tf.train.Feature(int64_list=tf.train.Int64List(value=xmin_list)),\n",
        "        'ymin': tf.train.Feature(int64_list=tf.train.Int64List(value=ymin_list)),\n",
        "        'xmax': tf.train.Feature(int64_list=tf.train.Int64List(value=xmax_list)),\n",
        "        'ymax': tf.train.Feature(int64_list=tf.train.Int64List(value=ymax_list)),\n",
        "    }))\n",
        "\n",
        "    return tf_example\n",
        "\n",
        "def create_tf_records(writer, preprocessed_image, page, output_path):\n",
        "    # sample output:\n",
        "    #   Page= 10 , page= {'@index': 10, '@width': 1654, '@height': 1170, 'frame': [{'@id': '000000cd', '@xmin': 341, '@ymin': 96, '@xmax': 485, '@ymax': 354, 'type': 'frame'}, {'@id': '000000cf', '@xmin': 834, '@ymin': 505, '@xmax': 1648, '@ymax': 745, 'type': 'frame'}, {'@id': '000000d3', '@xmin': 897, '@ymin': 750, '@xmax': 1216, '@ymax': 1169, 'type': 'frame'}, {'@id': '000000d5', '@xmin': 80, '@ymin': 721, '@xmax': 748, '@ymax': 1098, 'type': 'frame'}, {'@id': '000000d6', '@xmin': 1098, '@ymin': 1, '@xmax': 1653, '@ymax': 502, 'type': 'frame'}, {'@id': '000000da', '@xmin': 1214, '@ymin': 746, '@xmax': 1565, '@ymax': 1096, 'type': 'frame'}, {'@id': '000000e1', '@xmin': 489, '@ymin': 3, '@xmax': 745, '@ymax': 360, 'type': 'frame'}, {'@id': '000000e2', '@xmin': 82, '@ymin': 96, '@xmax': 340, '@ymax': 356, 'type': 'frame'}, {'@id': '000000e3', '@xmin': 1, '@ymin': 372, '@xmax': 817, '@ymax': 720, 'type': 'frame'}, {'@id': '000000e4', '@xmin': 901, '@ymin': 100, '@xmax': 1092, '@ymax': 502, 'type': 'frame'}], 'face': [{'@id': '000000d0', '@xmin': 987, '@ymin': 163, '@xmax': 1029, '@ymax': 192, '@character': '00000010', 'type': 'face'}, {'@id': '000000d4', '@xmin': 350, '@ymin': 150, '@xmax': 471, '@ymax': 242, '@character': '00000003', 'type': 'face'}, {'@id': '000000d9', '@xmin': 1043, '@ymin': 775, '@xmax': 1088, '@ymax': 808, '@character': '00000010', 'type': 'face'}], 'body': [{'@id': '000000ce', '@xmin': 595, '@ymin': 129, '@xmax': 662, '@ymax': 227, '@character': '00000010', 'type': 'body'}, {'@id': '000000d1', '@xmin': 343, '@ymin': 100, '@xmax': 480, '@ymax': 351, '@character': '00000003', 'type': 'body'}, {'@id': '000000d7', '@xmin': 969, '@ymin': 138, '@xmax': 1050, '@ymax': 367, '@character': '00000010', 'type': 'body'}, {'@id': '000000d8', '@xmin': 991, '@ymin': 752, '@xmax': 1148, '@ymax': 1062, '@character': '00000010', 'type': 'body'}, {'@id': '000000db', '@xmin': 1310, '@ymin': 515, '@xmax': 1623, '@ymax': 743, '@character': '00000090', 'type': 'body'}, {'@id': '000000dd', '@xmin': 139, '@ymin': 385, '@xmax': 817, '@ymax': 723, '@character': '00000090', 'type': 'body'}, {'@id': '000000df', '@xmin': 383, '@ymin': 372, '@xmax': 477, '@ymax': 459, '@character': '00000010', 'type': 'body'}],\n",
        "    #       'text': [\n",
        "    #           {'@id': '000000d2', '@xmin': 698, '@ymin': 238, '@xmax': 711, '@ymax': 284, '#text': 'ブン？', 'type': 'text'},\n",
        "    #           {'@id': '000000dc', '@xmin': 356, '@ymin': 273, '@xmax': 403, '@ymax': 340, '#text': 'あの人.....', 'type': 'text'},\n",
        "    #           {'@id': '000000de', '@xmin': 1131, '@ymin': 752, '@xmax': 1200, '@ymax': 841, '#text': 'わっ', 'type': 'text'},\n",
        "    #           {'@id': '000000e0', '@xmin': 482, '@ymin': 91, '@xmax': 498, '@ymax': 145, '#text': 'あれ？', 'type': 'text'}]}\n",
        "    text_rects = page.get('text')\n",
        "\n",
        "    tf_result = create_tf_manga109_rects_from_page(\n",
        "        preprocessed_image, page['@width'], page['@height'],\n",
        "        text_rects)\n",
        "    writer.write(tf_result.SerializeToString())\n",
        "\n",
        "\n",
        "# Note that writer will append to the file if it already exists, but rather than having it open/close\n",
        "# we'll just open it once here and close it at the end of the script\n",
        "tf_record_paths = os.path.join(tf_model_dir, 'manga109_detection.tfrecords')\n",
        "tf_record_writer = tf.io.TFRecordWriter(tf_record_paths, 'a')   # 'a' for append (if file exists)   \n",
        "\n",
        "# we want to continue on from where we left of, so look at the last book processed\n",
        "last_book_processed = ''\n",
        "bookmark_paths = os.path.join(tf_model_dir, 'manga109_detection_bookmark.txt')\n",
        "if os.path.exists(bookmark_paths):\n",
        "    with open(bookmark_paths, 'r') as bookmark_file:\n",
        "        last_book_processed = bookmark_file.read()\n",
        "        bookmark_file.close()\n",
        "\n",
        "# clone books list, so that we can pop/skip books we've already processed\n",
        "book_lists = manga109.books.copy()\n",
        "while book_lists:\n",
        "    if last_book_processed == book_lists[0] || last_book_processed == '':\n",
        "        break\n",
        "    book_lists.pop(0)\n",
        "\n",
        "# Example usage:\n",
        "# annotation_data = {\n",
        "#    'YumeiroCooking/000.jpg': [{'class': 'body', 'xmin': 100, 'ymin': 50, 'xmax': 200, 'ymax': 150}],\n",
        "#    'YumeiroCooking/001.jpg': [{'class': 'face', 'xmin': 50, 'ymin': 30, 'xmax': 100, 'ymax': 80}],\n",
        "# }\n",
        "# Iterate through all books\n",
        "for book in book_lists:\n",
        "    print(f\"Processing book: {book}\")\n",
        "    annotations_of_this_book = manga109.get_annotation(book)\n",
        "    pages = annotations_of_this_book['page']\n",
        "\n",
        "    # Iterate through all pages in the book\n",
        "    # sample output:\n",
        "    #   Page= 0 , page= {'@index': 0, '@width': 1654, '@height': 1170, 'frame': [], 'face': [], 'body': [], 'text': []}\n",
        "    #   Page= 2 , page= {'@index': 2, '@width': 1654, '@height': 1170, 'frame': [{'@id': '00000000', '@xmin': 83, '@ymin': 86, '@xmax': 751, '@ymax': 1090, 'type': 'frame'}], 'face': [{'@id': '00000004', '@xmin': 406, '@ymin': 684, '@xmax': 456, '@ymax': 764, '@character': '00000003', 'type': 'face'}], 'body': [{'@id': '00000002', '@xmin': 178, '@ymin': 660, '@xmax': 548, '@ymax': 965, '@character': '00000003', 'type': 'body'}], 'text': [{'@id': '00000001', '@xmin': 550, '@ymin': 660, '@xmax': 583, '@ymax': 696, '#text': 'あ', 'type': 'text'}]}\n",
        "    #   Page= 7 , page= {'@index': 7, '@width': 1654, '@height': 1170, 'frame': [{'@id': '0000007e', '@xmin': 53, '@ymin': 6, '@xmax': 419, '@ymax': 361, 'type': 'frame'}, {'@id': '00000086', '@xmin': 901, '@ymin': 93, '@xmax': 1382, '@ymax': 500, 'type': 'frame'}, {'@id': '00000088', '@xmin': 901, '@ymin': 519, '@xmax': 1567, '@ymax': 1169, 'type': 'frame'}, {'@id': '0000008c', '@xmin': 435, '@ymin': 98, '@xmax': 747, '@ymax': 361, 'type': 'frame'}, {'@id': '0000008d', '@xmin': 5, '@ymin': 361, '@xmax': 819, '@ymax': 1169, 'type': 'frame'}, {'@id': '0000008e', '@xmin': 1385, '@ymin': 96, '@xmax': 1565, '@ymax': 501, 'type': 'frame'}], 'face': [{'@id': '0000007a', '@xmin': 223, '@ymin': 199, '@xmax': 348, '@ymax': 286, '@character': '00000003', 'type': 'face'}, {'@id': '0000007f', '@xmin': 1117, '@ymin': 204, '@xmax': 1231, '@ymax': 294, '@character': '00000003', 'type': 'face'}, {'@id': '00000080', '@xmin': 1403, '@ymin': 449, '@xmax': 1454, '@ymax': 494, '@character': '00000010', 'type': 'face'}, {'@id': '00000083', '@xmin': 492, '@ymin': 276, '@xmax': 541, '@ymax': 316, '@character': '00000010', 'type': 'face'}], 'body': [{'@id': '00000077', '@xmin': 431, '@ymin': 249, '@xmax': 597, '@ymax': 363, '@character': '00000010', 'type': 'body'}, {'@id': '00000079', '@xmin': 1400, '@ymin': 444, '@xmax': 1458, '@ymax': 501, '@character': '00000010', 'type': 'body'}, {'@id': '00000081', '@xmin': 161, '@ymin': 91, '@xmax': 419, '@ymax': 363, '@character': '00000003', 'type': 'body'}, {'@id': '00000087', '@xmin': 1043, '@ymin': 114, '@xmax': 1364, '@ymax': 501, '@character': '00000003', 'type': 'body'}, {'@id': '0000008f', '@xmin': 37, '@ymin': 415, '@xmax': 766, '@ymax': 1012, '@character': '00000090', 'type': 'body'}], 'text': [{'@id': '00000078', '@xmin': 463, '@ymin': 695, '@xmax': 477, '@ymax': 736, '#text': 'しょうぶ.....', 'type': 'text'}, {'@id': '0000007b', '@xmin': 217, '@ymin': 348, '@xmax': 268, '@ymax': 456, '#text': 'こらっ\\nこのやろ', 'type': 'text'}, {'@id': '0000007c', '@xmin': 55, '@ymin': 251, '@xmax': 95, '@ymax': 334, '#text': 'おいっ', 'type': 'text'}, {'@id': '0000007d', '@xmin': 693, '@ymin': 92, '@xmax': 749, '@ymax': 178, '#text': '出てこいっ！', 'type': 'text'}, {'@id': '00000082', '@xmin': 1284, '@ymin': 78, '@xmax': 1380, '@ymax': 300, '#text': 'そこかっ！', 'type': 'text'}, {'@id': '00000084', '@xmin': 573, '@ymin': 260, '@xmax': 622, '@ymax': 316, '#text': 'むちゃ言うな', 'type': 'text'}, {'@id': '00000085', '@xmin': 397, '@ymin': 90, '@xmax': 414, '@ymax': 173, '#text': 'どこだ！', 'type': 'text'}, {'@id': '00000089', '@xmin': 327, '@ymin': 723, '@xmax': 374, '@ymax': 772, '#text': 'なんちゃって\\nはは....', 'type': 'text'}, {'@id': '0000008a', '@xmin': 532, '@ymin': 483, '@xmax': 631, '@ymax': 660, '#text': 'おいっ......てばっ\\n出てきてわたしと', 'type': 'text'}, {'@id': '0000008b', '@xmin': 89, '@ymin': 85, '@xmax': 175, '@ymax': 203, '#text': '出て来て私と勝負しろっ！', 'type': 'text'}]}\n",
        "    #   Page= 10 , page= {'@index': 10, '@width': 1654, '@height': 1170, 'frame': [{'@id': '000000cd', '@xmin': 341, '@ymin': 96, '@xmax': 485, '@ymax': 354, 'type': 'frame'}, {'@id': '000000cf', '@xmin': 834, '@ymin': 505, '@xmax': 1648, '@ymax': 745, 'type': 'frame'}, {'@id': '000000d3', '@xmin': 897, '@ymin': 750, '@xmax': 1216, '@ymax': 1169, 'type': 'frame'}, {'@id': '000000d5', '@xmin': 80, '@ymin': 721, '@xmax': 748, '@ymax': 1098, 'type': 'frame'}, {'@id': '000000d6', '@xmin': 1098, '@ymin': 1, '@xmax': 1653, '@ymax': 502, 'type': 'frame'}, {'@id': '000000da', '@xmin': 1214, '@ymin': 746, '@xmax': 1565, '@ymax': 1096, 'type': 'frame'}, {'@id': '000000e1', '@xmin': 489, '@ymin': 3, '@xmax': 745, '@ymax': 360, 'type': 'frame'}, {'@id': '000000e2', '@xmin': 82, '@ymin': 96, '@xmax': 340, '@ymax': 356, 'type': 'frame'}, {'@id': '000000e3', '@xmin': 1, '@ymin': 372, '@xmax': 817, '@ymax': 720, 'type': 'frame'}, {'@id': '000000e4', '@xmin': 901, '@ymin': 100, '@xmax': 1092, '@ymax': 502, 'type': 'frame'}], 'face': [{'@id': '000000d0', '@xmin': 987, '@ymin': 163, '@xmax': 1029, '@ymax': 192, '@character': '00000010', 'type': 'face'}, {'@id': '000000d4', '@xmin': 350, '@ymin': 150, '@xmax': 471, '@ymax': 242, '@character': '00000003', 'type': 'face'}, {'@id': '000000d9', '@xmin': 1043, '@ymin': 775, '@xmax': 1088, '@ymax': 808, '@character': '00000010', 'type': 'face'}], 'body': [{'@id': '000000ce', '@xmin': 595, '@ymin': 129, '@xmax': 662, '@ymax': 227, '@character': '00000010', 'type': 'body'}, {'@id': '000000d1', '@xmin': 343, '@ymin': 100, '@xmax': 480, '@ymax': 351, '@character': '00000003', 'type': 'body'}, {'@id': '000000d7', '@xmin': 969, '@ymin': 138, '@xmax': 1050, '@ymax': 367, '@character': '00000010', 'type': 'body'}, {'@id': '000000d8', '@xmin': 991, '@ymin': 752, '@xmax': 1148, '@ymax': 1062, '@character': '00000010', 'type': 'body'}, {'@id': '000000db', '@xmin': 1310, '@ymin': 515, '@xmax': 1623, '@ymax': 743, '@character': '00000090', 'type': 'body'}, {'@id': '000000dd', '@xmin': 139, '@ymin': 385, '@xmax': 817, '@ymax': 723, '@character': '00000090', 'type': 'body'}, {'@id': '000000df', '@xmin': 383, '@ymin': 372, '@xmax': 477, '@ymax': 459, '@character': '00000010', 'type': 'body'}], 'text': [{'@id': '000000d2', '@xmin': 698, '@ymin': 238, '@xmax': 711, '@ymax': 284, '#text': 'ブン？', 'type': 'text'}, {'@id': '000000dc', '@xmin': 356, '@ymin': 273, '@xmax': 403, '@ymax': 340, '#text': 'あの人.....', 'type': 'text'}, {'@id': '000000de', '@xmin': 1131, '@ymin': 752, '@xmax': 1200, '@ymax': 841, '#text': 'わっ', 'type': 'text'}, {'@id': '000000e0', '@xmin': 482, '@ymin': 91, '@xmax': 498, '@ymax': 145, '#text': 'あれ？', 'type': 'text'}]}\n",
        "    # NOTE: each page can have multiple text regions\n",
        "    for page_index, page in enumerate(pages):\n",
        "        # print(\"\\tPage=\", page_index, \", page=\", page)\n",
        "\n",
        "        # Load image using Manga109 API - the annotation provided may mismatch in a sense of some JPG may be missing,\n",
        "        # causing premature exit of the loop, hence we'll try to verify if the file exists first and skip\n",
        "        if not os.path.exists(manga109.img_path(book=book, index=page_index)):\n",
        "            print(\"File not found: \", manga109.img_path(\n",
        "                book=book, index=page_index))\n",
        "            continue\n",
        "\n",
        "        # Load image using Manga109 API\n",
        "        image = Image.open(manga109.img_path(book=book, index=page_index))\n",
        "        image_flattened = tf.keras.preprocessing.image.img_to_array(image)\n",
        "        preprocessed = tf.keras.applications.efficientnet.preprocess_input(\n",
        "            image_flattened)\n",
        "\n",
        "        # Display preprocessed image (optional)\n",
        "        # import matplotlib.pyplot as plt\n",
        "        # plt.imshow(preprocessed)\n",
        "        # plt.axis('off')\n",
        "        # plt.show()\n",
        "\n",
        "        frame_rects = page.get('frame')\n",
        "        face_rects = page.get('face')\n",
        "        body_rects = page.get('body')\n",
        "        text_rects = page.get('text')\n",
        "\n",
        "        # We're ONLY interested in classifying text regions\n",
        "        if text_rects.__len__() > 0:\n",
        "            create_tf_records(tf_record_writer, preprocessed, page, tf_record_paths)\n",
        "\n",
        "            # for debugging, if we find pages with texts, print them\n",
        "            print(page_index, end=\" \")\n",
        "    tf_record_writer.flush()\n",
        "\n",
        "    # preserve the last book processed\n",
        "    last_book_processed = book\n",
        "    with open(bookmark_paths, 'w') as bookmark_file:\n",
        "        bookmark_file.write(last_book_processed)\n",
        "        bookmark_file.close()\n",
        "\n",
        "tf_record_writer.close()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Model and Loss Function:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#!/usr/bin/env python\n",
        "from tensorflow.keras.layers import Input, Conv2D, MaxPooling2D, Flatten, Dense\n",
        "from tensorflow.keras.models import Model\n",
        "\n",
        "# Define your model architecture\n",
        "global model\n",
        "\n",
        "\n",
        "def create_model(input_shape, num_classes):\n",
        "    inputs = Input(shape=input_shape)\n",
        "    x = Conv2D(32, (3, 3), activation='relu')(inputs)\n",
        "    x = MaxPooling2D((2, 2))(x)\n",
        "    x = Conv2D(64, (3, 3), activation='relu')(x)\n",
        "    x = MaxPooling2D((2, 2))(x)\n",
        "    x = Flatten()(x)\n",
        "    x = Dense(128, activation='relu')(x)\n",
        "    outputs = Dense(num_classes, activation='softmax')(x)\n",
        "\n",
        "    model = Model(inputs=inputs, outputs=outputs)\n",
        "    return model\n",
        "\n",
        "\n",
        "# Compile the model\n",
        "model = create_model(input_shape=(224, 224, 3), num_classes=4)\n",
        "model.compile(optimizer='adam', loss='categorical_crossentropy',\n",
        "              metrics=['accuracy'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Data Augmentation:\n",
        "This is possibly not needed since I am now using TFRecord...\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#!/usr/bin/env python\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "\n",
        "global datagen\n",
        "# Define data augmentation parameters\n",
        "datagen = ImageDataGenerator(\n",
        "    rotation_range=10,\n",
        "    width_shift_range=0.1,\n",
        "    height_shift_range=0.1,\n",
        "    zoom_range=0.1,\n",
        "    horizontal_flip=True,\n",
        "    vertical_flip=True,\n",
        "    fill_mode='nearest'\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Training...\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#!/usr/bin/env python\n",
        "# Train the model\n",
        "BATCH_SIZE = 32\n",
        "steps_per_epoch = len(tf_record_paths) // BATCH_SIZE\n",
        "EPOCHS = 10\n",
        "SHUFFLE_BUFFER_SIZE = 10000\n",
        "#\n",
        "#  # Convert annotations to one-hot encoding for classes\n",
        "#  # Assuming you have a function to convert class labels to numeric IDs\n",
        "#  # For simplicity, let's assume classes are already numeric IDs\n",
        "#  # If not, you need to convert them using label encoding\n",
        "#  # Also, you need to modify your create_tf_example function to include numeric class labels\n",
        "#  # Assuming you have label_map as a dictionary mapping class labels to numeric IDs\n",
        "#  # label_map = {'body': 0, 'face': 1, 'frame': 2, 'text': 3}\n",
        "#\n",
        "#  # Define steps per epoch based on the number of training samples and batch size\n",
        "#  steps_per_epoch = len(annotation_data) // BATCH_SIZE\n",
        "#\n",
        "\n",
        "\n",
        "\n",
        "#  # Train the model using fit_generator\n",
        "#  model.fit_generator(\n",
        "#      datagen.flow_from_directory(\n",
        "#          '/path/to/dataset', target_size=(224, 224), batch_size=BATCH_SIZE),\n",
        "#      steps_per_epoch=steps_per_epoch,\n",
        "#      epochs=EPOCHS\n",
        "#  )\n",
        "\n",
        "\n",
        "def parse_tfrecord_fn(example):\n",
        "    feature_description = {\n",
        "        'image': tf.io.FixedLenFeature([], tf.string),\n",
        "        'xmin': tf.io.VarLenFeature(tf.int64),\n",
        "        'ymin': tf.io.VarLenFeature(tf.int64),\n",
        "        'xmax': tf.io.VarLenFeature(tf.int64),\n",
        "        'ymax': tf.io.VarLenFeature(tf.int64),\n",
        "        'page_width': tf.io.FixedLenFeature([], tf.int64),\n",
        "        'page_height': tf.io.FixedLenFeature([], tf.int64),\n",
        "    }\n",
        "    example = tf.io.parse_single_example(example, feature_description)\n",
        "\n",
        "    image = tf.io.decode_raw(example['image'], tf.uint8)\n",
        "    image = tf.reshape(\n",
        "        image, [example['page_height'], example['page_width'], -1])\n",
        "    xmin = tf.sparse.to_dense(example['xmin'])\n",
        "    ymin = tf.sparse.to_dense(example['ymin'])\n",
        "    xmax = tf.sparse.to_dense(example['xmax'])\n",
        "    ymax = tf.sparse.to_dense(example['ymax'])\n",
        "\n",
        "    return image, xmin, ymin, xmax, ymax\n",
        "\n",
        "\n",
        "# Load TFRecord data\n",
        "raw_dataset = tf.data.TFRecordDataset(tf_record_paths)\n",
        "\n",
        "# Parse the data\n",
        "parsed_dataset = raw_dataset.map(parse_tfrecord_fn)\n",
        "\n",
        "# Shuffle and batch the data\n",
        "dataset_from_tfrecord = parsed_dataset.shuffle(\n",
        "    SHUFFLE_BUFFER_SIZE).batch(BATCH_SIZE)\n",
        "\n",
        "# Convert the tf.data.Dataset to a generator\n",
        "\n",
        "\n",
        "def gen_from_tfrecords():\n",
        "    for features in dataset_from_tfrecord:\n",
        "        yield features\n",
        "\n",
        "\n",
        "# if previous models exists, load the weights\n",
        "if os.path.exists(os.path.join(tf_model_dir, 'manga109_detection.weights.h5')):\n",
        "    model.load_weights(os.path.join(\n",
        "        tf_model_dir, 'manga109_detection.weights.h5'))\n",
        "\n",
        "# Train the model using fit_generator\n",
        "# model.fit_generator(gen(), steps_per_epoch=len(tf_record_paths) // BATCH_SIZE, epochs=EPOCHS)\n",
        "model.fit_generator(\n",
        "    # datagen.flow_from_directory( '/path/to/dataset', target_size=(224, 224), batch_size=BATCH_SIZE),\n",
        "    gen_from_tfrecords(),\n",
        "    steps_per_epoch=steps_per_epoch,\n",
        "    epochs=EPOCHS\n",
        ")\n",
        "\n",
        "# And now, we need to save it\n",
        "model.save_weights(os.path.join(tf_model_dir, 'manga109_detection.weights.h5'))\n",
        "# entire model including arch, optimizer, and training config:\n",
        "model.save(os.path.join(tf_model_dir, 'manga109_detection.model.h5'))"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "authorship_tag": "ABX9TyMJa4AZntSaTgDIhORKwCOR",
      "include_colab_link": true,
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
