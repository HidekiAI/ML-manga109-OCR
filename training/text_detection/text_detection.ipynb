{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/HidekiAI/ML-manga109-OCR/blob/trunk/Untitled0.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "First two are essential, but not necessarily needed for both CoLab and local Jupyter-notebook. But without these, when you crash or restart, you cannot skip it... For CoLab, you must first make sure remote drive is mounted. To align BASH and Python scripts to work on multiple platform, for local, you'd need to either soft-link (or junction) and/or mount (i.e. `mount bind`).\n",
        "\n",
        "Note that below is ONLY necessary for Google CoLab to access your Google Drive. If on Notepad/Jupyter, do the following instead (not exact, just the example):\n",
        "\n",
        "-   Linux: make sure to `ln -sv ~/Google/MyDrive /content/drive` to softlink your Google G-Drive as `/content/drive`\n",
        "-   Windows: From DOS Command Prompt (right clock to launch as Admin) `mklink.exe /D \"C:/content/drive\" \"C:/Users/HidekiAI/Google/MyDrive/\"` to create a dir-junction\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#!/usr/bin/python\n",
        "# No need to execute this if running locally, this is only for Google CoLab usage\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Next, we'll need the (official) tools/libraries to read manga109 (annotation) data from https://github.com/manga109. This is essential to both CoLab and local dev'ing.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#!/bin/bash\n",
        "# MUST run ths on BOTH CoLab and local...\n",
        "!pip install manga109api"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "I want to know which version of TF is installed, I cannot run GPU version on my local machine... If it returns empty array '[]' for both CPU and GPU, then you'd need to do the next step first and come back here. If you do verify you have either CPU or GPU installed, you can skip most of the diagnostic-checks for TensorFlow and go straight to the script where it defines the globals for src and dest data dirs.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "!/usr/bin/env python\n",
        "# Optionally run this to check the TensorFlow version and configuration\n",
        "import tensorflow as tf\n",
        "\n",
        "# Check TensorFlow version\n",
        "print(\"TensorFlow version:\", tf.__version__)\n",
        "\n",
        "# Check TensorFlow configuration\n",
        "print(\"TensorFlow configuration:\")\n",
        "print(tf.config.list_physical_devices('GPU'))  # List available GPUs\n",
        "print(tf.config.list_physical_devices('CPU'))  # List available CPUs"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We want to make sure TensorFlow is installed in the Python (virtual) environment for local setup...\n",
        "\n",
        "-   TensorFlow Object Detection is now depracated\n",
        "-   TensorFlow Addons (for using TF-Vision) sunsets on May, 2024 and needs to be switched over to Keras, in which it should be accessible directly as long as TF is installed\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#!/bin/bash\n",
        "# NOTE: NO NEED to run this on CoLab, only on local...\n",
        "!pip install --upgrade pip\n",
        "\n",
        "!pip install -U --pre tensorflow==\"2.*\"\n",
        "!pip install tensorflow\n",
        "# Comment above and uncomment below if you want to install tensorflow-gpu instead of tensorflow on CoLab\n",
        "#!pip install tensorflow-gpu\n",
        "#pip install tensorflow[and-cuda]\n",
        "\n",
        "!pip install transformers\n",
        "!pip install tf-models-official\n",
        "!pip install tf-keras-vis"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Next, I'd like to absolutely make sure we have access to TF-Vision for text detection; Because tensorflow-addons has become sunset as of May, 2024, we just need to verify that keras is accessible...\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#!/usr/bin/env python\n",
        "# Optionally run this to check the TensorFlow version and configuration\n",
        "from tensorflow.keras.layers import Dense, Conv2D, Flatten, MaxPooling2D\n",
        "from tensorflow.keras.models import Sequential\n",
        "import tensorflow as tf\n",
        "\n",
        "# Check TensorFlow version\n",
        "print(\"TensorFlow version:\", tf.__version__)\n",
        "\n",
        "def create_model(input_shape, num_classes):\n",
        "    inputs = Input(shape=input_shape)\n",
        "    x = Conv2D(32, (3, 3), activation='relu')(inputs)\n",
        "    x = MaxPooling2D((2, 2))(x)\n",
        "    x = Conv2D(64, (3, 3), activation='relu')(x)\n",
        "    x = MaxPooling2D((2, 2))(x)\n",
        "    x = Flatten()(x)\n",
        "    x = Dense(128, activation='relu')(x)\n",
        "    outputs = Dense(num_classes, activation='softmax')(x)\n",
        "\n",
        "    model = Model(inputs=inputs, outputs=outputs)\n",
        "    return model\n",
        "\n",
        "def create_ssd_model(num_classes, image_size=(224, 224), weights='imagenet', include_top=False):\n",
        "    base_model = MobileNetV2(input_shape=(image_size[0], image_size[1], 3), weights=weights, include_top=include_top)\n",
        "\n",
        "    for layer in base_model.layers:\n",
        "        layer.trainable = False\n",
        "\n",
        "    ssd_output = layers.Conv2D(num_classes, kernel_size=(1, 1), activation='softmax')(base_model.output)\n",
        "\n",
        "    model = models.Model(inputs=base_model.input, outputs=ssd_output)\n",
        "\n",
        "    return model\n",
        "\n",
        "# Access Keras functionality through tf.keras\n",
        "# Define a simple Sequential model\n",
        "test_keras_model = Sequential([\n",
        "    Conv2D(16, 3, padding='same', activation='relu', input_shape=(32, 32, 3)),\n",
        "    MaxPooling2D(),\n",
        "    Conv2D(32, 3, padding='same', activation='relu'),\n",
        "    MaxPooling2D(),\n",
        "    Conv2D(64, 3, padding='same', activation='relu'),\n",
        "    MaxPooling2D(),\n",
        "    Flatten(),\n",
        "    Dense(128, activation='relu'),\n",
        "    Dense(10, activation='softmax')\n",
        "])\n",
        "\n",
        "# Compile the model\n",
        "test_keras_model.compile(optimizer='adam',\n",
        "                         loss='sparse_categorical_crossentropy',\n",
        "                         metrics=['accuracy'])\n",
        "\n",
        "# Print model summary\n",
        "test_keras_model.summary()\n",
        "\n",
        "# Also should verify model creations based on what I am using later...\n",
        "test_keras_model = create_model(input_shape=(224, 224, 3), num_classes=4)\n",
        "test_keras_model.compile(optimizer='adam', loss='categorical_crossentropy',\n",
        "                         metrics=['accuracy'])\n",
        "test_keras_model.summary()\n",
        "\n",
        "test_keras_model = create_ssd_model(NUM_CLASSES, image_size=(224, 224), weights='imagenet', include_top=False)\n",
        "test_keras_model.compile(optimizer='adam', loss='categorical_crossentropy',\n",
        "                         metrics=['accuracy'])\n",
        "test_keras_model.summary()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Once TF-Vision is loaded, let's verify for sure via Python...\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#!/usr/bin/env python\n",
        "# Optionally run this to check the TensorFlow version and configuration\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.applications import EfficientNetB0\n",
        "\n",
        "\n",
        "# Check TensorFlow version\n",
        "print(\"TensorFlow version:\", tf.__version__)\n",
        "\n",
        "# Try importing a TensorFlow Vision model (e.g., EfficientNet)\n",
        "try:\n",
        "    # Import the EfficientNetB0 model\n",
        "    test_keras_model = EfficientNetB0(weights='imagenet')\n",
        "    print(\"TensorFlow Vision (via Keras) is accessible.\")\n",
        "\n",
        "except ImportError:\n",
        "    print(\"TensorFlow Vision (via Keras) is not accessible.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Verify either via BASH or python that we can access `/content/drive` mount\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#!/bin/bash\n",
        "! pwd && [ -e /content/drive/MyDrive ] || echo \"Unable to validate Google Drive from bash script\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#!/usr/bin/env python\n",
        "import os\n",
        "\n",
        "# directory path to the Manga109 dataset (read-only)\n",
        "global manga109_dir\n",
        "# directory path to the TensorFlow TFRecord model (read-write)\n",
        "global tf_model_dir\n",
        "\n",
        "# Check if Google Drive is mounted and/or locally have symlink (or junctions) to access '/content/drive/MyDrive'\n",
        "if os.path.isdir('/content/drive'):\n",
        "    # list contents of the root directory of Google drive\n",
        "    # change this to your own path\n",
        "    root_paths = '/content/drive/MyDrive/projects/ML-manga-ocr-rust/'\n",
        "    data_paths = os.path.join(root_paths, 'data/')  # should pre-exist!\n",
        "    tf_model_dir = os.path.join(data_paths, 'tf_model/')\n",
        "    # mkdir if not exists\n",
        "    if not os.path.exists(tf_model_dir):\n",
        "        os.makedirs(tf_model_dir)\n",
        "        print('Created TensorFlow model directory at ', tf_model_dir)\n",
        "\n",
        "    drive_files = os.listdir(root_paths)\n",
        "    print(drive_files)\n",
        "    drive_files = os.listdir(data_paths)\n",
        "    print(drive_files)\n",
        "    zip_path = os.path.join(root_paths, 'data/Manga109s.zip')\n",
        "    if os.path.exists(zip_path):\n",
        "        # only UNZIP IF dir does not exist, else assume it's already unzipped\n",
        "        if not os.path.exists(data_paths):\n",
        "            # os.makedirs(data_paths)\n",
        "            #!unzip '{zip_path}' -d '{data_paths}'\n",
        "            with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
        "                zip_ref.extractall(data_paths)\n",
        "                print('Unzipped the data to ', data_paths)\n",
        "    drive_files = os.listdir(data_paths)\n",
        "    manga109_dir = os.path.join(\n",
        "        data_paths, 'Manga109s/Manga109s_released_2023_12_07/')\n",
        "    data_dir_files = os.listdir(manga109_dir)\n",
        "    print(data_dir_files)\n",
        "    # lastly, notify users of their license by printing the readme.txt\n",
        "    readme_path = os.path.join(manga109_dir, 'readme.txt')\n",
        "    with open(readme_path, 'r', encoding=\"utf-8\") as bookmark_file:\n",
        "        print(bookmark_file.read())\n",
        "else:\n",
        "    print(\"Google Drive is not mounted.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Now that we have manga dir accessible, let's try out the manga109api...\n",
        "\n",
        "NOTE: See also https://github.com/manga109/manga109-demos/tree/master/visualization, which is basically the same thing but I'm using PyPlot..."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#!/usr/bin/env python\n",
        "import manga109api\n",
        "from PIL import Image, ImageDraw\n",
        "\n",
        "\n",
        "def draw_rectangle(img, x0, y0, x1, y1, annotation_type):\n",
        "    assert annotation_type in [\"body\", \"face\", \"frame\", \"text\"]\n",
        "    color = {\"body\": \"#258039\", \"face\": \"#f5be41\",\n",
        "             \"frame\": \"#31a9b8\", \"text\": \"#cf3721\"}[annotation_type]\n",
        "    draw = ImageDraw.Draw(img)\n",
        "    draw.rectangle([x0, y0, x1, y1], outline=color, width=10)\n",
        "\n",
        "test_book = \"YumeiroCooking\"\n",
        "page_index = 6\n",
        "\n",
        "p = manga109api.Parser(root_dir=manga109_dir)\n",
        "annotation = p.get_annotation(book=test_book)\n",
        "img = Image.open(p.img_path(book=test_book, index=page_index))\n",
        "\n",
        "for annotation_type in [\"body\", \"face\", \"frame\", \"text\"]:\n",
        "    rois = annotation[\"page\"][page_index][annotation_type]\n",
        "    for roi in rois:\n",
        "        draw_rectangle(img, roi[\"@xmin\"], roi[\"@ymin\"],\n",
        "                        roi[\"@xmax\"], roi[\"@ymax\"], annotation_type)\n",
        "\n",
        "# Display preprocessed image\n",
        "import matplotlib.pyplot as plt\n",
        "plt.imshow(img)\n",
        "plt.axis('off')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Load and Preprocess Images with TensorFlow:\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "If you did see an image load up with rectangles around texts, you are now ready to integrate it with TF-Vision..."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#!/usr/bin/env python\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import tensorflow as tf\n",
        "import manga109api\n",
        "from PIL import Image, ImageDraw\n",
        "\n",
        "# Initialize Manga109 API\n",
        "manga109 = manga109api.Parser(root_dir=manga109_dir)\n",
        "\n",
        "# Choose a manga volume and page index\n",
        "test_volume = 'YumeiroCooking'\n",
        "page_index = 6\n",
        "\n",
        "# Load image using Manga109 API\n",
        "test_image = Image.open(manga109.img_path(book=test_volume, index=page_index))\n",
        "\n",
        "# Preprocess image using TensorFlow Keras\n",
        "test_image = tf.keras.preprocessing.image.img_to_array(test_image)\n",
        "test_image = tf.keras.applications.efficientnet.preprocess_input(test_image)\n",
        "\n",
        "# Display preprocessed image\n",
        "plt.imshow(test_image)\n",
        "plt.axis('off')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "If the above worked for single book/volume, we can now iterate the ENTIRE books it knows about; There is a minor issue in which curated annotation file thinks there is a JPG associated to it, in which the images dir for that book no longer exists, so we'll have to do extra checks (extra I/O means performance) whether the file exists or not.\n",
        "We'll preprocess image prior to making it into TFRecord. Ideally, we'd want this to be on a separate cell, but it causes memory outage due to huge blocks of images, hence we'll check if image has text-regions, and if so, create a TFRecord for that region\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Model and Loss Function:\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Data Augmentation:\n",
        "This is possibly not needed since I am now using TFRecord...\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Training...\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Import libraries\n",
        "import os\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers, models\n",
        "from tensorflow.keras.applications import MobileNetV2\n",
        "from tensorflow.keras.applications.mobilenet_v2 import preprocess_input\n",
        "from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "import numpy as np\n",
        "\n",
        "# Define constants\n",
        "IMAGE_SIZE = (224, 224)\n",
        "NUM_CLASSES = 2  # Text and background\n",
        "BATCH_SIZE = 32\n",
        "EPOCHS = 10\n",
        "LEARNING_RATE = 0.001\n",
        "\n",
        "# Define paths to data\n",
        "train_images_dir = os.path.join(manga109_dir, 'images')\n",
        "train_annotations_dir = os.paths.join(manga109_dir, 'annotations')\n",
        "\n",
        "# Define data generator\n",
        "datagen = tf.keras.preprocessing.image.ImageDataGenerator(\n",
        "    preprocessing_function=preprocess_input,\n",
        "    rescale=1. / 255,\n",
        "    validation_split=0.2)\n",
        "\n",
        "train_generator = datagen.flow_from_directory(\n",
        "    train_images_dir,\n",
        "    target_size=IMAGE_SIZE,\n",
        "    batch_size=BATCH_SIZE,\n",
        "    class_mode='binary',    # or class_mode='categorical' if you have more than two classes\n",
        "    subset='training')\n",
        "\n",
        "val_generator = datagen.flow_from_directory(\n",
        "    train_images_dir,\n",
        "    target_size=IMAGE_SIZE,\n",
        "    batch_size=BATCH_SIZE,\n",
        "    class_mode='binary',    # or class_mode='categorical' if you have more than two classes\n",
        "    subset='validation')\n",
        "\n",
        "# Define SSD model\n",
        "def create_ssd_model(num_classes, image_size=(224, 224), weights='imagenet', include_top=False):\n",
        "    base_model = MobileNetV2(input_shape=(image_size[0], image_size[1], 3), weights=weights, include_top=include_top)\n",
        "\n",
        "    for layer in base_model.layers:\n",
        "        layer.trainable = False\n",
        "\n",
        "    ssd_output = layers.Conv2D(num_classes, kernel_size=(1, 1), activation='softmax')(base_model.output)\n",
        "\n",
        "    model = models.Model(inputs=base_model.input, outputs=ssd_output)\n",
        "\n",
        "    return model\n",
        "\n",
        "\n",
        "# Compile model\n",
        "model = create_ssd_model(NUM_CLASSES, image_size=(224, 224), weights='imagenet', include_top=False)\n",
        "optimizer = Adam(learning_rate=LEARNING_RATE)\n",
        "model.compile(optimizer=optimizer, loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# Define callbacks\n",
        "checkpoint_callback = ModelCheckpoint(filepath=tf_model_dir, save_weights_only=True, save_best_only=True,\n",
        "                                      monitor='val_loss', mode='min', verbose=1)\n",
        "early_stopping_callback = EarlyStopping(monitor='val_loss', patience=3, mode='min', verbose=1)\n",
        "\n",
        "# Train model\n",
        "history = model.fit(train_generator, epochs=EPOCHS, validation_data=val_generator,\n",
        "                    callbacks=[checkpoint_callback, early_stopping_callback])\n",
        "\n",
        "# Evaluate model\n",
        "test_images_dir = os.path.join(manga109_dir, 'test/images')\n",
        "test_annotations_dir = os.path.join(manga109_dir, '/test/annotations')\n",
        "\n",
        "test_generator = datagen.flow_from_directory(\n",
        "    test_images_dir,\n",
        "    target_size=IMAGE_SIZE,\n",
        "    batch_size=BATCH_SIZE,\n",
        "    class_mode='binary')\n",
        "\n",
        "loss, accuracy = model.evaluate(test_generator)\n",
        "print(\"Test Loss:\", loss)\n",
        "print(\"Test Accuracy:\", accuracy)\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "authorship_tag": "ABX9TyMJa4AZntSaTgDIhORKwCOR",
      "include_colab_link": true,
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
