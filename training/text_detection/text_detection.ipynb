{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/HidekiAI/ML-manga109-OCR/blob/trunk/Untitled0.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "S5BXth-cnCwW"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Defaulting to user installation because normal site-packages is not writeable\n",
            "Requirement already satisfied: tensorflow==2.* in c:\\users\\hidekiai\\appdata\\roaming\\python\\python311\\site-packages (2.16.1)\n",
            "Requirement already satisfied: tensorflow-intel==2.16.1 in c:\\users\\hidekiai\\appdata\\roaming\\python\\python311\\site-packages (from tensorflow==2.*) (2.16.1)\n",
            "Requirement already satisfied: absl-py>=1.0.0 in c:\\users\\hidekiai\\appdata\\roaming\\python\\python311\\site-packages (from tensorflow-intel==2.16.1->tensorflow==2.*) (2.1.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in c:\\users\\hidekiai\\appdata\\roaming\\python\\python311\\site-packages (from tensorflow-intel==2.16.1->tensorflow==2.*) (1.6.3)\n",
            "Requirement already satisfied: flatbuffers>=23.5.26 in c:\\users\\hidekiai\\appdata\\roaming\\python\\python311\\site-packages (from tensorflow-intel==2.16.1->tensorflow==2.*) (24.3.25)\n",
            "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in c:\\users\\hidekiai\\appdata\\roaming\\python\\python311\\site-packages (from tensorflow-intel==2.16.1->tensorflow==2.*) (0.5.4)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in c:\\users\\hidekiai\\appdata\\roaming\\python\\python311\\site-packages (from tensorflow-intel==2.16.1->tensorflow==2.*) (0.2.0)\n",
            "Requirement already satisfied: h5py>=3.10.0 in c:\\users\\hidekiai\\appdata\\roaming\\python\\python311\\site-packages (from tensorflow-intel==2.16.1->tensorflow==2.*) (3.11.0)\n",
            "Requirement already satisfied: libclang>=13.0.0 in c:\\users\\hidekiai\\appdata\\roaming\\python\\python311\\site-packages (from tensorflow-intel==2.16.1->tensorflow==2.*) (18.1.1)\n",
            "Requirement already satisfied: ml-dtypes~=0.3.1 in c:\\users\\hidekiai\\appdata\\roaming\\python\\python311\\site-packages (from tensorflow-intel==2.16.1->tensorflow==2.*) (0.3.2)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in c:\\users\\hidekiai\\appdata\\roaming\\python\\python311\\site-packages (from tensorflow-intel==2.16.1->tensorflow==2.*) (3.3.0)\n",
            "Requirement already satisfied: packaging in c:\\programdata\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.16.1->tensorflow==2.*) (23.1)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in c:\\programdata\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.16.1->tensorflow==2.*) (3.20.3)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.16.1->tensorflow==2.*) (2.31.0)\n",
            "Requirement already satisfied: setuptools in c:\\programdata\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.16.1->tensorflow==2.*) (68.2.2)\n",
            "Requirement already satisfied: six>=1.12.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.16.1->tensorflow==2.*) (1.16.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.16.1->tensorflow==2.*) (2.4.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in c:\\programdata\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.16.1->tensorflow==2.*) (4.9.0)\n",
            "Requirement already satisfied: wrapt>=1.11.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.16.1->tensorflow==2.*) (1.14.1)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in c:\\users\\hidekiai\\appdata\\roaming\\python\\python311\\site-packages (from tensorflow-intel==2.16.1->tensorflow==2.*) (1.64.0rc1)\n",
            "Requirement already satisfied: tensorboard<2.17,>=2.16 in c:\\users\\hidekiai\\appdata\\roaming\\python\\python311\\site-packages (from tensorflow-intel==2.16.1->tensorflow==2.*) (2.16.2)\n",
            "Requirement already satisfied: keras>=3.0.0 in c:\\users\\hidekiai\\appdata\\roaming\\python\\python311\\site-packages (from tensorflow-intel==2.16.1->tensorflow==2.*) (3.3.3)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in c:\\users\\hidekiai\\appdata\\roaming\\python\\python311\\site-packages (from tensorflow-intel==2.16.1->tensorflow==2.*) (0.31.0)\n",
            "Requirement already satisfied: numpy<2.0.0,>=1.23.5 in c:\\programdata\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.16.1->tensorflow==2.*) (1.26.4)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from astunparse>=1.6.0->tensorflow-intel==2.16.1->tensorflow==2.*) (0.41.2)\n",
            "Requirement already satisfied: rich in c:\\programdata\\anaconda3\\lib\\site-packages (from keras>=3.0.0->tensorflow-intel==2.16.1->tensorflow==2.*) (13.3.5)\n",
            "Requirement already satisfied: namex in c:\\users\\hidekiai\\appdata\\roaming\\python\\python311\\site-packages (from keras>=3.0.0->tensorflow-intel==2.16.1->tensorflow==2.*) (0.0.8)\n",
            "Requirement already satisfied: optree in c:\\users\\hidekiai\\appdata\\roaming\\python\\python311\\site-packages (from keras>=3.0.0->tensorflow-intel==2.16.1->tensorflow==2.*) (0.11.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests<3,>=2.21.0->tensorflow-intel==2.16.1->tensorflow==2.*) (2.0.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests<3,>=2.21.0->tensorflow-intel==2.16.1->tensorflow==2.*) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests<3,>=2.21.0->tensorflow-intel==2.16.1->tensorflow==2.*) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests<3,>=2.21.0->tensorflow-intel==2.16.1->tensorflow==2.*) (2024.2.2)\n",
            "Requirement already satisfied: markdown>=2.6.8 in c:\\programdata\\anaconda3\\lib\\site-packages (from tensorboard<2.17,>=2.16->tensorflow-intel==2.16.1->tensorflow==2.*) (3.4.1)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in c:\\users\\hidekiai\\appdata\\roaming\\python\\python311\\site-packages (from tensorboard<2.17,>=2.16->tensorflow-intel==2.16.1->tensorflow==2.*) (0.7.2)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from tensorboard<2.17,>=2.16->tensorflow-intel==2.16.1->tensorflow==2.*) (2.2.3)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from werkzeug>=1.0.1->tensorboard<2.17,>=2.16->tensorflow-intel==2.16.1->tensorflow==2.*) (2.1.3)\n",
            "Requirement already satisfied: markdown-it-py<3.0.0,>=2.2.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from rich->keras>=3.0.0->tensorflow-intel==2.16.1->tensorflow==2.*) (2.2.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from rich->keras>=3.0.0->tensorflow-intel==2.16.1->tensorflow==2.*) (2.15.1)\n",
            "Requirement already satisfied: mdurl~=0.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from markdown-it-py<3.0.0,>=2.2.0->rich->keras>=3.0.0->tensorflow-intel==2.16.1->tensorflow==2.*) (0.1.0)\n",
            "Defaulting to user installation because normal site-packages is not writeable\n",
            "Collecting tf-models-official\n",
            "  Using cached tf_models_official-2.16.0-py2.py3-none-any.whl.metadata (1.4 kB)\n",
            "Collecting Cython (from tf-models-official)\n",
            "  Using cached Cython-3.0.10-cp311-cp311-win_amd64.whl.metadata (3.2 kB)\n",
            "Requirement already satisfied: Pillow in c:\\programdata\\anaconda3\\lib\\site-packages (from tf-models-official) (10.2.0)\n",
            "Collecting gin-config (from tf-models-official)\n",
            "  Using cached gin_config-0.5.0-py3-none-any.whl.metadata (2.9 kB)\n",
            "Collecting google-api-python-client>=1.6.7 (from tf-models-official)\n",
            "  Using cached google_api_python_client-2.129.0-py2.py3-none-any.whl.metadata (6.7 kB)\n",
            "Collecting immutabledict (from tf-models-official)\n",
            "  Using cached immutabledict-4.2.0-py3-none-any.whl.metadata (3.4 kB)\n",
            "Collecting kaggle>=1.3.9 (from tf-models-official)\n",
            "  Using cached kaggle-1.6.12.tar.gz (79 kB)\n",
            "  Preparing metadata (setup.py): started\n",
            "  Preparing metadata (setup.py): finished with status 'done'\n",
            "Requirement already satisfied: matplotlib in c:\\programdata\\anaconda3\\lib\\site-packages (from tf-models-official) (3.8.0)\n",
            "Requirement already satisfied: numpy>=1.20 in c:\\programdata\\anaconda3\\lib\\site-packages (from tf-models-official) (1.26.4)\n",
            "Collecting oauth2client (from tf-models-official)\n",
            "  Using cached oauth2client-4.1.3-py2.py3-none-any.whl.metadata (1.2 kB)\n",
            "Requirement already satisfied: opencv-python-headless in c:\\programdata\\anaconda3\\lib\\site-packages (from tf-models-official) (4.9.0.80)\n",
            "Requirement already satisfied: pandas>=0.22.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from tf-models-official) (2.1.4)\n",
            "Requirement already satisfied: psutil>=5.4.3 in c:\\programdata\\anaconda3\\lib\\site-packages (from tf-models-official) (5.9.0)\n",
            "Requirement already satisfied: py-cpuinfo>=3.3.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from tf-models-official) (9.0.0)\n",
            "Collecting pycocotools (from tf-models-official)\n",
            "  Using cached pycocotools-2.0.7-cp311-cp311-win_amd64.whl.metadata (1.1 kB)\n",
            "Requirement already satisfied: pyyaml>=6.0.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from tf-models-official) (6.0.1)\n",
            "Collecting sacrebleu (from tf-models-official)\n",
            "  Using cached sacrebleu-2.4.2-py3-none-any.whl.metadata (58 kB)\n",
            "Requirement already satisfied: scipy>=0.19.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from tf-models-official) (1.11.4)\n",
            "Collecting sentencepiece (from tf-models-official)\n",
            "  Using cached sentencepiece-0.2.0-cp311-cp311-win_amd64.whl.metadata (8.3 kB)\n",
            "Collecting seqeval (from tf-models-official)\n",
            "  Using cached seqeval-1.2.2.tar.gz (43 kB)\n",
            "  Preparing metadata (setup.py): started\n",
            "  Preparing metadata (setup.py): finished with status 'done'\n",
            "Requirement already satisfied: six in c:\\programdata\\anaconda3\\lib\\site-packages (from tf-models-official) (1.16.0)\n",
            "Collecting tensorflow-datasets (from tf-models-official)\n",
            "  Using cached tensorflow_datasets-4.9.4-py3-none-any.whl.metadata (9.2 kB)\n",
            "Collecting tensorflow-hub>=0.6.0 (from tf-models-official)\n",
            "  Using cached tensorflow_hub-0.16.1-py2.py3-none-any.whl.metadata (1.3 kB)\n",
            "Collecting tensorflow-model-optimization>=0.4.1 (from tf-models-official)\n",
            "  Using cached tensorflow_model_optimization-0.8.0-py2.py3-none-any.whl.metadata (904 bytes)\n",
            "INFO: pip is looking at multiple versions of tf-models-official to determine which version is compatible with other requirements. This could take a while.\n",
            "Collecting tf-models-official\n",
            "  Using cached tf_models_official-2.15.0-py2.py3-none-any.whl.metadata (1.4 kB)\n",
            "  Using cached tf_models_official-2.14.2-py2.py3-none-any.whl.metadata (1.4 kB)\n",
            "  Using cached tf_models_official-2.14.1-py2.py3-none-any.whl.metadata (1.4 kB)\n",
            "  Using cached tf_models_official-2.14.0-py2.py3-none-any.whl.metadata (1.4 kB)\n",
            "  Using cached tf_models_official-2.13.2-py2.py3-none-any.whl.metadata (1.4 kB)\n",
            "  Using cached tf_models_official-2.13.1-py2.py3-none-any.whl.metadata (1.4 kB)\n",
            "Collecting pyyaml<5.4.0,>=5.1 (from tf-models-official)\n",
            "  Using cached PyYAML-5.3.1.tar.gz (269 kB)\n",
            "  Preparing metadata (setup.py): started\n",
            "  Preparing metadata (setup.py): finished with status 'done'\n",
            "Collecting tf-models-official\n",
            "  Using cached tf_models_official-2.13.0-py2.py3-none-any.whl.metadata (1.4 kB)\n",
            "Collecting pyyaml<6.0,>=5.1 (from tf-models-official)\n",
            "  Using cached PyYAML-5.4.1.tar.gz (175 kB)\n",
            "  Installing build dependencies: started\n",
            "  Installing build dependencies: finished with status 'done'\n",
            "  Getting requirements to build wheel: started\n",
            "  Getting requirements to build wheel: finished with status 'error'\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  error: subprocess-exited-with-error\n",
            "  \n",
            "  × Getting requirements to build wheel did not run successfully.\n",
            "  │ exit code: 1\n",
            "  ╰─> [54 lines of output]\n",
            "      running egg_info\n",
            "      writing lib3\\PyYAML.egg-info\\PKG-INFO\n",
            "      writing dependency_links to lib3\\PyYAML.egg-info\\dependency_links.txt\n",
            "      writing top-level names to lib3\\PyYAML.egg-info\\top_level.txt\n",
            "      Traceback (most recent call last):\n",
            "        File \"C:\\ProgramData\\anaconda3\\Lib\\site-packages\\pip\\_vendor\\pyproject_hooks\\_in_process\\_in_process.py\", line 353, in <module>\n",
            "          main()\n",
            "        File \"C:\\ProgramData\\anaconda3\\Lib\\site-packages\\pip\\_vendor\\pyproject_hooks\\_in_process\\_in_process.py\", line 335, in main\n",
            "          json_out['return_val'] = hook(**hook_input['kwargs'])\n",
            "                                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "        File \"C:\\ProgramData\\anaconda3\\Lib\\site-packages\\pip\\_vendor\\pyproject_hooks\\_in_process\\_in_process.py\", line 118, in get_requires_for_build_wheel\n",
            "          return hook(config_settings)\n",
            "                 ^^^^^^^^^^^^^^^^^^^^^\n",
            "        File \"C:\\Users\\HidekiAI\\AppData\\Local\\Temp\\pip-build-env-735zfgi0\\overlay\\Lib\\site-packages\\setuptools\\build_meta.py\", line 325, in get_requires_for_build_wheel\n",
            "          return self._get_build_requires(config_settings, requirements=['wheel'])\n",
            "                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "        File \"C:\\Users\\HidekiAI\\AppData\\Local\\Temp\\pip-build-env-735zfgi0\\overlay\\Lib\\site-packages\\setuptools\\build_meta.py\", line 295, in _get_build_requires\n",
            "          self.run_setup()\n",
            "        File \"C:\\Users\\HidekiAI\\AppData\\Local\\Temp\\pip-build-env-735zfgi0\\overlay\\Lib\\site-packages\\setuptools\\build_meta.py\", line 311, in run_setup\n",
            "          exec(code, locals())\n",
            "        File \"<string>\", line 271, in <module>\n",
            "        File \"C:\\Users\\HidekiAI\\AppData\\Local\\Temp\\pip-build-env-735zfgi0\\overlay\\Lib\\site-packages\\setuptools\\__init__.py\", line 104, in setup\n",
            "          return distutils.core.setup(**attrs)\n",
            "                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "        File \"C:\\Users\\HidekiAI\\AppData\\Local\\Temp\\pip-build-env-735zfgi0\\overlay\\Lib\\site-packages\\setuptools\\_distutils\\core.py\", line 184, in setup\n",
            "          return run_commands(dist)\n",
            "                 ^^^^^^^^^^^^^^^^^^\n",
            "        File \"C:\\Users\\HidekiAI\\AppData\\Local\\Temp\\pip-build-env-735zfgi0\\overlay\\Lib\\site-packages\\setuptools\\_distutils\\core.py\", line 200, in run_commands\n",
            "          dist.run_commands()\n",
            "        File \"C:\\Users\\HidekiAI\\AppData\\Local\\Temp\\pip-build-env-735zfgi0\\overlay\\Lib\\site-packages\\setuptools\\_distutils\\dist.py\", line 969, in run_commands\n",
            "          self.run_command(cmd)\n",
            "        File \"C:\\Users\\HidekiAI\\AppData\\Local\\Temp\\pip-build-env-735zfgi0\\overlay\\Lib\\site-packages\\setuptools\\dist.py\", line 967, in run_command\n",
            "          super().run_command(command)\n",
            "        File \"C:\\Users\\HidekiAI\\AppData\\Local\\Temp\\pip-build-env-735zfgi0\\overlay\\Lib\\site-packages\\setuptools\\_distutils\\dist.py\", line 988, in run_command\n",
            "          cmd_obj.run()\n",
            "        File \"C:\\Users\\HidekiAI\\AppData\\Local\\Temp\\pip-build-env-735zfgi0\\overlay\\Lib\\site-packages\\setuptools\\command\\egg_info.py\", line 321, in run\n",
            "          self.find_sources()\n",
            "        File \"C:\\Users\\HidekiAI\\AppData\\Local\\Temp\\pip-build-env-735zfgi0\\overlay\\Lib\\site-packages\\setuptools\\command\\egg_info.py\", line 329, in find_sources\n",
            "          mm.run()\n",
            "        File \"C:\\Users\\HidekiAI\\AppData\\Local\\Temp\\pip-build-env-735zfgi0\\overlay\\Lib\\site-packages\\setuptools\\command\\egg_info.py\", line 550, in run\n",
            "          self.add_defaults()\n",
            "        File \"C:\\Users\\HidekiAI\\AppData\\Local\\Temp\\pip-build-env-735zfgi0\\overlay\\Lib\\site-packages\\setuptools\\command\\egg_info.py\", line 588, in add_defaults\n",
            "          sdist.add_defaults(self)\n",
            "        File \"C:\\Users\\HidekiAI\\AppData\\Local\\Temp\\pip-build-env-735zfgi0\\overlay\\Lib\\site-packages\\setuptools\\command\\sdist.py\", line 102, in add_defaults\n",
            "          super().add_defaults()\n",
            "        File \"C:\\Users\\HidekiAI\\AppData\\Local\\Temp\\pip-build-env-735zfgi0\\overlay\\Lib\\site-packages\\setuptools\\_distutils\\command\\sdist.py\", line 250, in add_defaults\n",
            "          self._add_defaults_ext()\n",
            "        File \"C:\\Users\\HidekiAI\\AppData\\Local\\Temp\\pip-build-env-735zfgi0\\overlay\\Lib\\site-packages\\setuptools\\_distutils\\command\\sdist.py\", line 335, in _add_defaults_ext\n",
            "          self.filelist.extend(build_ext.get_source_files())\n",
            "                               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "        File \"<string>\", line 201, in get_source_files\n",
            "        File \"C:\\Users\\HidekiAI\\AppData\\Local\\Temp\\pip-build-env-735zfgi0\\overlay\\Lib\\site-packages\\setuptools\\_distutils\\cmd.py\", line 107, in __getattr__\n",
            "          raise AttributeError(attr)\n",
            "      AttributeError: cython_sources\n",
            "      [end of output]\n",
            "  \n",
            "  note: This error originates from a subprocess, and is likely not a problem with pip.\n",
            "error: subprocess-exited-with-error\n",
            "\n",
            "× Getting requirements to build wheel did not run successfully.\n",
            "│ exit code: 1\n",
            "╰─> See above for output.\n",
            "\n",
            "note: This error originates from a subprocess, and is likely not a problem with pip.\n"
          ]
        }
      ],
      "source": [
        "#!/bin/bash\n",
        "!pip install -U --pre tensorflow==\"2.*\"\n",
        "!pip install tf-models-official"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "/c/Users/HidekiAI/projects/remote/github/ML-manga109-OCR/training/text_detection\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Cloning into 'models'...\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "/c/Users/HidekiAI/projects/remote/github/ML-manga109-OCR/training/text_detection\n"
          ]
        }
      ],
      "source": [
        "#!/bin/bash\n",
        "# Clone the TensorFlow Models repository\n",
        "!pwd && [ -e models ] || git clone https://github.com/tensorflow/models.git\n",
        "\n",
        "# Install the Object Detection API\n",
        "!pwd && [ -e models ] && cd models/research \\\n",
        "&& pwd \\\n",
        "&& protoc object_detection/protos/*.proto --python_out=. \\\n",
        "&& cp object_detection/packages/tf2/setup.py . \\\n",
        "&& python -m pip install .python -m pip install ."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "ename": "ModuleNotFoundError",
          "evalue": "No module named 'google.colab'",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[2], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mgoogle\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcolab\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m drive\n\u001b[0;32m      2\u001b[0m drive\u001b[38;5;241m.\u001b[39mmount(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m/content/drive\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
            "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'google.colab'"
          ]
        }
      ],
      "source": [
        "# No need to execute this if running locally, this is only for Google CoLab usage\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "/c/Users/HidekiAI/projects/remote/github/ML-manga109-OCR/training/text_detection\n",
            "\"Unable to validate Google Drive from bash script\"\n",
            "['data', 'text_detection.ipynb', 'test.txt', 'desktop.ini']\n",
            "['Untitled0.ipynb', 'Manga109s', 'desktop.ini']\n",
            "['books.txt', 'readme.txt', 'annotations', 'annotations.v2018.05.31', 'annotations.v2020.12.18', 'images', 'desktop.ini']\n",
            "Users are requested to strictly observe the following rules when using Manga109-s;\n",
            "\n",
            "1.Redistribution of the Manga109-s dataset to third parties is forbidden.\n",
            "2.When publishing results (including pre-trained models) obtained from machine learning experiments or image processing experiments, the use of the Manga109-s dataset must be indicated clearly within the published work.\n",
            "3.Selling manga images within the dataset together with results obtained from machine learning experiments or image processing experiments is forbidden.\n",
            "4.Direct copies or modifications of the manga images within the Manga109-s dataset must not be treated as products, regardless of the product being either free or being sold for a fee.\n",
            "5.When publishing whole pages (or modifications of whole pages) of the manga works contained within the dataset for the purpose of presenting the results of research and development, the total number of whole pages (including modifications of whole pages) to be published must not exceed 20% of the entire book (volume), for each of the books (volumes) in the dataset. Publishing over 20% of whole pages or modifications of whole pages of any book (volume) is forbidden.\n",
            "6.When including manga material in an academic paper or video, users should include the relevant author’s name as “courtesy of [Author’s Name]”.\n",
            "7.When including manga extracts in an academic paper or video, users are requested to acknowledge their use of Manga109-s. In addition, when quoting Manga109 material in an academic paper, users are requested to cite the related paper. Titles:\n",
            "@article{multimedia_aizawa_2020,\n",
            "    author={Kiyoharu Aizawa and Azuma Fujimoto and Atsushi Otsubo and Toru Ogawa and Yusuke Matsui and Koki Tsubota and Hikaru Ikuta},\n",
            "    title={Building a Manga Dataset ``Manga109'' with Annotations for Multimedia Applications},\n",
            "    journal={IEEE MultiMedia},\n",
            "    volume={27},\n",
            "    number={2},\n",
            "    pages={8--18},\n",
            "    doi={10.1109/mmul.2020.2987895},\n",
            "    year={2020}\n",
            "}\n",
            "@article{mtap_matsui_2017,\n",
            "    author={Yusuke Matsui and Kota Ito and Yuji Aramaki and Azuma Fujimoto and Toru Ogawa and Toshihiko Yamasaki and Kiyoharu Aizawa},\n",
            "    title={Sketch-based Manga Retrieval using Manga109 Dataset},\n",
            "    journal={Multimedia Tools and Applications},\n",
            "    volume={76},\n",
            "    number={20},\n",
            "    pages={21811--21838},\n",
            "    doi={10.1007/s11042-016-4020-z},\n",
            "    year={2017}\n",
            "}\n",
            "\n",
            "Manga109 contains comics which are approximately 40 years old and may contain images that are considered inappropriate based on modern day global ethics. Please be careful upon its use.\n",
            "\n",
            "Folder Structure\n",
            "- images/: images\n",
            "- annotations/: annotations of the latest version (v2021.12.30)\n",
            "- annotations.v2020.12.18/: annotations of the old version\n",
            "- annotations.v2018.05.31/: annotations of the old version\n",
            "\n",
            "----------------------------------------------------------------------\n",
            "\n",
            "本データセットを使用するにあたって、以下の項目を遵守してください.\n",
            "\n",
            "\n",
            "1.第三者にデータを譲渡しないこと.\n",
            "2.機械学習実験及び画像処理実験で得た結果物（学習済みモデル等を含む）の公開にあたっては，Manga109-sデータセットの利用を明記すること．\n",
            "3.機械学習実験及び画像処理実験で得た結果物と漫画画像をセットにした形で作品の販売をしないこと．\n",
            "4.Manga109-s内の漫画画像の複製および改変を有料，無料を問わず商材としてはならない．\n",
            "5.研究開発の成果を示すために，漫画の頁あるいはその改変を公開する場合には，1巻当たり20％をこえた分量を公開してはならない．\n",
            "6.利用した画像を論文, ビデオなどで使用するときは, 当該漫画の作者名を「(作者名)提供」のように明示すること．\n",
            "7.論文での掲載にあたっては, Manga109-sのデータであることを明示し, 関連論文の引用をすること. 論文:\n",
            "@article{multimedia_aizawa_2020,\n",
            "    author={Kiyoharu Aizawa and Azuma Fujimoto and Atsushi Otsubo and Toru Ogawa and Yusuke Matsui and Koki Tsubota and Hikaru Ikuta},\n",
            "    title={Building a Manga Dataset ``Manga109'' with Annotations for Multimedia Applications},\n",
            "    journal={IEEE MultiMedia},\n",
            "    volume={27},\n",
            "    number={2},\n",
            "    pages={8--18},\n",
            "    doi={10.1109/mmul.2020.2987895},\n",
            "    year={2020}\n",
            "}\n",
            "@article{mtap_matsui_2017,\n",
            "    author={Yusuke Matsui and Kota Ito and Yuji Aramaki and Azuma Fujimoto and Toru Ogawa and Toshihiko Yamasaki and Kiyoharu Aizawa},\n",
            "    title={Sketch-based Manga Retrieval using Manga109 Dataset},\n",
            "    journal={Multimedia Tools and Applications},\n",
            "    volume={76},\n",
            "    number={20},\n",
            "    pages={21811--21838},\n",
            "    doi={10.1007/s11042-016-4020-z},\n",
            "    year={2017}\n",
            "}\n",
            "\n",
            "Manga109はおよそ40年前の漫画も扱っており，近年の世界的な倫理観では不適切な画像も含まれることがあります．その利用には注意してください．\n",
            "\n",
            "フォルダ構成\n",
            "- images/: 画像\n",
            "- annotations/: 最新版のアノテーション（v2021.12.30）\n",
            "- annotations.v2020.12.18/: 過去版のアノテーション\n",
            "- annotations.v2018.05.31/: 過去版のアノテーション\n",
            "\n"
          ]
        }
      ],
      "source": [
        "! pwd && [ -e /content/drive/MyDrive ] || echo \"Unable to validate Google Drive from bash script\"\n",
        "\n",
        "import os\n",
        "\n",
        "# Check if Google Drive is mounted and/or locally have symlink (or junctions) to access '/content/drive/MyDrive'\n",
        "if os.path.isdir('/content/drive'):\n",
        "  #list contents of the root directory of Google drive\n",
        "  root_paths = '/content/drive/MyDrive/projects/ML-manga-ocr-rust/'\n",
        "  drive_files = os.listdir(root_paths)\n",
        "  print(drive_files)\n",
        "  data_paths = os.path.join(root_paths, 'data/')\n",
        "  drive_files = os.listdir(data_paths)\n",
        "  print(drive_files)\n",
        "  zip_path = os.path.join(root_paths, 'data/Manga109s.zip')\n",
        "  if os.path.exists(zip_path):\n",
        "    # only UNZIP IF dir does not exist, else assume it's already unzipped\n",
        "    if not os.path.exists(data_paths):\n",
        "      #os.makedirs(data_paths)\n",
        "      #!unzip '{zip_path}' -d '{data_paths}'\n",
        "      with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
        "        zip_ref.extractall(data_paths)\n",
        "        print('Unzipped the data to ', data_paths)\n",
        "  drive_files = os.listdir(data_paths)\n",
        "  manga109_dir = os.path.join(data_paths, 'Manga109s/Manga109s_released_2023_12_07/')\n",
        "  data_dir_files = os.listdir(manga109_dir)\n",
        "  print(data_dir_files)\n",
        "  # lastly, notify users of their license by printing the readme.txt\n",
        "  readme_path = os.path.join(manga109_dir, 'readme.txt')\n",
        "  with open(readme_path, 'r') as file:\n",
        "    print(file.read())\n",
        "else:\n",
        "  print(\"Google Drive is not mounted.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensorflow (2.16.1) is installed\n",
            "object_detection is NOT installed\n",
            "pycocotools is NOT installed\n",
            "lvis is NOT installed\n",
            "numpy (1.26.4) is installed\n",
            "matplotlib (3.8.0) is installed\n",
            "cython is NOT installed\n",
            "contextlib2 is NOT installed\n",
            "tf_slim is NOT installed\n",
            "six (1.16.0) is installed\n",
            "Pillow (10.2.0) is installed\n",
            "lxml (4.9.3) is installed\n",
            "jupyter (1.0.0) is installed\n",
            "tensorflow-addons is NOT installed\n",
            "tf-models-official is NOT installed\n",
            "Protobuf is installed\n",
            "TensorFlow Object Detection API is NOT installed\n"
          ]
        }
      ],
      "source": [
        "# Python\n",
        "import os\n",
        "import pkg_resources\n",
        "\n",
        "REQUIRED_PACKAGES = [\n",
        "    'tensorflow',\n",
        "    'object_detection',\n",
        "    'pycocotools',\n",
        "    'lvis',\n",
        "    'numpy',\n",
        "    'matplotlib',\n",
        "    'cython',\n",
        "    'contextlib2',\n",
        "    'tf_slim',\n",
        "    'six',\n",
        "    'Pillow',\n",
        "    'lxml',\n",
        "    'jupyter',\n",
        "    'tensorflow-addons',\n",
        "    'tf-models-official'\n",
        "]\n",
        "\n",
        "for package in REQUIRED_PACKAGES:\n",
        "    try:\n",
        "        dist = pkg_resources.get_distribution(package)\n",
        "        print('{} ({}) is installed'.format(package, dist.version))\n",
        "    except pkg_resources.DistributionNotFound:\n",
        "        print('{} is NOT installed'.format(package))\n",
        "\n",
        "# Check Protobuf installation\n",
        "if os.system('protoc') == 0:\n",
        "    print('Protobuf is installed')\n",
        "else:\n",
        "    print('Protobuf is NOT installed')\n",
        "\n",
        "# Check TensorFlow Object Detection API installation\n",
        "try:\n",
        "    from object_detection.utils import label_map_util\n",
        "    print('TensorFlow Object Detection API is installed')\n",
        "except ImportError:\n",
        "    print('TensorFlow Object Detection API is NOT installed, see https://github.com/tensorflow/models/tree/master/research/object_detection for more details.')"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "authorship_tag": "ABX9TyMJa4AZntSaTgDIhORKwCOR",
      "include_colab_link": true,
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
