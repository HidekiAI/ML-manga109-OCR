{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/HidekiAI/ML-manga109-OCR/blob/trunk/Untitled0.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "First two are essential, but not necessarily needed for both CoLab and local Jupyter-notebook. But without these, when you crash or restart, you cannot skip it... For CoLab, you must first make sure remote drive is mounted. To align BASH and Python scripts to work on multiple platform, for local, you'd need to either soft-link (or junction) and/or mount (i.e. `mount bind`).\n",
        "\n",
        "Note that below is ONLY necessary for Google CoLab to access your Google Drive. If on Notepad/Jupyter, do the following instead (not exact, just the example):\n",
        "\n",
        "-   Linux: make sure to `ln -sv ~/Google/MyDrive /content/drive` to softlink your Google G-Drive as `/content/drive`\n",
        "-   Windows: From DOS Command Prompt (right clock to launch as Admin) `mklink.exe /D \"C:/content/drive\" \"C:/Users/HidekiAI/Google/MyDrive/\"` to create a dir-junction\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#!/usr/bin/python\n",
        "# No need to execute this if running locally, this is only for Google CoLab usage\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Next, we'll need the (official) tools/libraries to read manga109 (annotation) data from https://github.com/manga109. This is essential to both CoLab and local dev'ing.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#!/bin/bash\n",
        "# MUST run ths on BOTH CoLab and local...\n",
        "!pip install manga109api"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "I want to know which version of TF is installed, I cannot run GPU version on my local machine... If it returns empty array '[]' for both CPU and GPU, then you'd need to do the next step first and come back here. If you do verify you have either CPU or GPU installed, you can skip most of the diagnostic-checks for TensorFlow and go straight to the script where it defines the globals for src and dest data dirs.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#!/usr/bin/env python\n",
        "# Optionally run this to check the TensorFlow version and configuration\n",
        "import tensorflow as tf\n",
        "\n",
        "# Check TensorFlow version\n",
        "print(\"TensorFlow version:\", tf.__version__)\n",
        "\n",
        "# Check TensorFlow configuration\n",
        "print(\"TensorFlow configuration:\")\n",
        "print(tf.config.list_physical_devices('GPU'))  # List available GPUs\n",
        "print(tf.config.list_physical_devices('CPU'))  # List available CPUs"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We want to make sure TensorFlow is installed in the Python (virtual) environment for local setup...\n",
        "\n",
        "-   TensorFlow Object Detection is now depracated\n",
        "-   TensorFlow Addons (for using TF-Vision) sunsets on May, 2024 and needs to be switched over to Keras, in which it should be accessible directly as long as TF is installed\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#!/bin/bash\n",
        "# NOTE: NO NEED to run this on CoLab, only on local...\n",
        "!pip install --upgrade pip\n",
        "\n",
        "!pip install -U --pre tensorflow==\"2.*\"\n",
        "!pip install tensorflow\n",
        "# Comment above and uncomment below if you want to install tensorflow-gpu instead of tensorflow on CoLab\n",
        "#!pip install tensorflow-gpu\n",
        "#pip install tensorflow[and-cuda]\n",
        "\n",
        "!pip install transformers\n",
        "!pip install tf-models-official\n",
        "!pip install tf-keras-vis"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Next, I'd like to absolutely make sure we have access to TF-Vision for text detection; Because tensorflow-addons has become sunset as of May, 2024, we just need to verify that keras is accessible...\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#!/usr/bin/env python\n",
        "# Optionally run this to check the TensorFlow version and configuration\n",
        "from tensorflow.keras.layers import Input, Dense, Conv2D, Flatten, MaxPooling2D\n",
        "from tensorflow.keras.models import Sequential, Model\n",
        "from keras.applications import MobileNetV2\n",
        "from keras import layers\n",
        "import tensorflow as tf\n",
        "\n",
        "# Check TensorFlow version\n",
        "print(\"TensorFlow version:\", tf.__version__)\n",
        "\n",
        "\n",
        "def create_model(input_shape, num_classes):\n",
        "    inputs = Input(shape=input_shape)\n",
        "    x = Conv2D(32, (3, 3), activation='relu')(inputs)\n",
        "    x = MaxPooling2D((2, 2))(x)\n",
        "    x = Conv2D(64, (3, 3), activation='relu')(x)\n",
        "    x = MaxPooling2D((2, 2))(x)\n",
        "    x = Flatten()(x)\n",
        "    x = Dense(128, activation='relu')(x)\n",
        "    outputs = Dense(num_classes, activation='softmax')(x)\n",
        "\n",
        "    model = Model(inputs=inputs, outputs=outputs)\n",
        "    return model\n",
        "\n",
        "\n",
        "def create_ssd_model(num_classes, image_size=(224, 224), weights='imagenet', include_top=False):\n",
        "    base_model = MobileNetV2(input_shape=(\n",
        "        image_size[0], image_size[1], 3), weights=weights, include_top=include_top)\n",
        "\n",
        "    for layer in base_model.layers:\n",
        "        layer.trainable = False\n",
        "\n",
        "    ssd_output = layers.Conv2D(num_classes, kernel_size=(\n",
        "        1, 1), activation='softmax')(base_model.output)\n",
        "\n",
        "    model = Model(inputs=base_model.input, outputs=ssd_output)\n",
        "\n",
        "    return model\n",
        "\n",
        "\n",
        "# Access Keras functionality through tf.keras\n",
        "# Define a simple Sequential model\n",
        "test_keras_model = Sequential([\n",
        "    Conv2D(16, 3, padding='same', activation='relu', input_shape=(32, 32, 3)),\n",
        "    MaxPooling2D(),\n",
        "    Conv2D(32, 3, padding='same', activation='relu'),\n",
        "    MaxPooling2D(),\n",
        "    Conv2D(64, 3, padding='same', activation='relu'),\n",
        "    MaxPooling2D(),\n",
        "    Flatten(),\n",
        "    Dense(128, activation='relu'),\n",
        "    Dense(10, activation='softmax')\n",
        "])\n",
        "\n",
        "# Compile the model\n",
        "test_keras_model.compile(optimizer='adam',\n",
        "                         loss='sparse_categorical_crossentropy',\n",
        "                         metrics=['accuracy'])\n",
        "\n",
        "# Print model summary\n",
        "test_keras_model.summary()\n",
        "\n",
        "# Also should verify model creations based on what I am using later...\n",
        "test_keras_model = create_model(input_shape=(224, 224, 3), num_classes=4)\n",
        "test_keras_model.compile(optimizer='adam', loss='categorical_crossentropy',\n",
        "                         metrics=['accuracy'])\n",
        "test_keras_model.summary()\n",
        "\n",
        "test_keras_model = create_ssd_model(4, image_size=(\n",
        "    224, 224), weights='imagenet', include_top=False)\n",
        "test_keras_model.compile(optimizer='adam', loss='categorical_crossentropy',\n",
        "                         metrics=['accuracy'])\n",
        "test_keras_model.summary()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Once TF-Vision is loaded, let's verify for sure via Python...\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#!/usr/bin/env python\n",
        "# Optionally run this to check the TensorFlow version and configuration\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.applications import EfficientNetB0\n",
        "\n",
        "\n",
        "# Check TensorFlow version\n",
        "print(\"TensorFlow version:\", tf.__version__)\n",
        "\n",
        "# Try importing a TensorFlow Vision model (e.g., EfficientNet)\n",
        "try:\n",
        "    # Import the EfficientNetB0 model\n",
        "    test_keras_model = EfficientNetB0(weights='imagenet')\n",
        "    print(\"TensorFlow Vision (via Keras) is accessible.\")\n",
        "\n",
        "except ImportError:\n",
        "    print(\"TensorFlow Vision (via Keras) is not accessible.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Verify either via BASH or python that we can access `/content/drive` mount\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#!/bin/bash\n",
        "! pwd && [ -e /content/drive/MyDrive ] || echo \"Unable to validate Google Drive from bash script\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#!/usr/bin/env python\n",
        "import os\n",
        "\n",
        "# directory path to the Manga109 dataset (read-only)\n",
        "global project_root_dir\n",
        "global manga109_dir\n",
        "# directory path to the TensorFlow TFRecord model (read-write)\n",
        "global tf_model_dir\n",
        "\n",
        "# Check if Google Drive is mounted and/or locally have symlink (or junctions) to access '/content/drive/MyDrive'\n",
        "if os.path.isdir('/content/drive'):\n",
        "    # list contents of the root directory of Google drive\n",
        "    # change this to your own path\n",
        "    project_root_dir = '/content/drive/MyDrive/projects/ML-manga-ocr-rust/'\n",
        "    drive_files = os.listdir(project_root_dir)\n",
        "    print(drive_files)\n",
        "\n",
        "    data_paths = os.path.join(project_root_dir, 'data/')  # should pre-exist!\n",
        "    drive_files = os.listdir(data_paths)\n",
        "    print(drive_files)\n",
        "\n",
        "    tf_model_dir = os.path.join(data_paths, 'tf_model/')\n",
        "    # mkdir if not exists\n",
        "    if not os.path.exists(tf_model_dir):\n",
        "        os.makedirs(tf_model_dir)\n",
        "        print('Created TensorFlow model directory at ', tf_model_dir)\n",
        "\n",
        "    zip_path = os.path.join(project_root_dir, 'data/Manga109s.zip')\n",
        "    if os.path.exists(zip_path):\n",
        "        # only UNZIP IF dir does not exist, else assume it's already unzipped\n",
        "        if not os.path.exists(data_paths):\n",
        "            # os.makedirs(data_paths)\n",
        "            #!unzip '{zip_path}' -d '{data_paths}'\n",
        "            with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
        "                zip_ref.extractall(data_paths)\n",
        "                print('Unzipped the data to ', data_paths)\n",
        "    drive_files = os.listdir(data_paths)\n",
        "    print(drive_files)\n",
        "\n",
        "    manga109_dir = os.path.join(\n",
        "        data_paths, 'Manga109s/Manga109s_released_2023_12_07/')\n",
        "    data_dir_files = os.listdir(manga109_dir)\n",
        "    print(data_dir_files)\n",
        "\n",
        "    # lastly, notify users of their license by printing the readme.txt\n",
        "    readme_path = os.path.join(manga109_dir, 'readme.txt')\n",
        "    with open(readme_path, 'r', encoding=\"utf-8\") as bookmark_file:\n",
        "        print(bookmark_file.read())\n",
        "else:\n",
        "    print(\"Google Drive is not mounted.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Now that we have manga dir accessible, let's try out the manga109api...\n",
        "\n",
        "NOTE: See also https://github.com/manga109/manga109-demos/tree/master/visualization, which is basically the same thing but I'm using PyPlot...\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#!/usr/bin/env python\n",
        "import matplotlib.pyplot as plt\n",
        "import manga109api\n",
        "from PIL import Image, ImageDraw\n",
        "\n",
        "\n",
        "def draw_rectangle(img, x0, y0, x1, y1, annotation_type):\n",
        "    assert annotation_type in [\"body\", \"face\", \"frame\", \"text\"]\n",
        "    color = {\"body\": \"#258039\", \"face\": \"#f5be41\",\n",
        "             \"frame\": \"#31a9b8\", \"text\": \"#cf3721\"}[annotation_type]\n",
        "    draw = ImageDraw.Draw(img)\n",
        "    draw.rectangle([x0, y0, x1, y1], outline=color, width=10)\n",
        "\n",
        "\n",
        "test_book = \"YumeiroCooking\"\n",
        "page_index = 6\n",
        "\n",
        "p = manga109api.Parser(root_dir=manga109_dir)\n",
        "annotation = p.get_annotation(book=test_book)\n",
        "img = Image.open(p.img_path(book=test_book, index=page_index))\n",
        "\n",
        "for annotation_type in [\"body\", \"face\", \"frame\", \"text\"]:\n",
        "    rois = annotation[\"page\"][page_index][annotation_type]\n",
        "    for roi in rois:\n",
        "        draw_rectangle(img, roi[\"@xmin\"], roi[\"@ymin\"],\n",
        "                       roi[\"@xmax\"], roi[\"@ymax\"], annotation_type)\n",
        "\n",
        "# Display preprocessed image\n",
        "plt.imshow(img)\n",
        "plt.axis('off')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Load and Preprocess Images with TensorFlow:\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "If you did see an image load up with rectangles around texts, you are now ready to integrate it with TF-Vision...\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#!/usr/bin/env python\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import tensorflow as tf\n",
        "import manga109api\n",
        "from PIL import Image, ImageDraw\n",
        "\n",
        "# Initialize Manga109 API\n",
        "manga109 = manga109api.Parser(root_dir=manga109_dir)\n",
        "\n",
        "# Choose a manga volume and page index\n",
        "test_volume = 'YumeiroCooking'\n",
        "page_index = 6\n",
        "\n",
        "# Load image using Manga109 API\n",
        "test_image = Image.open(manga109.img_path(book=test_volume, index=page_index))\n",
        "\n",
        "# Preprocess image using TensorFlow Keras\n",
        "test_image = tf.keras.preprocessing.image.img_to_array(test_image)\n",
        "test_image = tf.keras.applications.efficientnet.preprocess_input(test_image)\n",
        "\n",
        "# Display preprocessed image\n",
        "plt.imshow(test_image)\n",
        "plt.axis('off')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "If the above worked for single book/volume, we can now iterate the ENTIRE books it knows about; There is a minor issue in which curated annotation file thinks there is a JPG associated to it, in which the images dir for that book no longer exists, so we'll have to do extra checks (extra I/O means performance) whether the file exists or not.\n",
        "We'll preprocess image prior to making it into TFRecord. Ideally, we'd want this to be on a separate cell, but it causes memory outage due to huge blocks of images, hence we'll check if image has text-regions, and if so, create a TFRecord for that region\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Import libraries\n",
        "import os\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers, models\n",
        "from tensorflow.keras.applications import MobileNetV2\n",
        "from tensorflow.keras.applications.mobilenet_v2 import preprocess_input\n",
        "from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "import numpy as np\n",
        "import keras\n",
        "\n",
        "global IMAGE_SIZE\n",
        "global BATCH_SIZE\n",
        "global EPOCHS\n",
        "global LEARNING_RATE\n",
        "global NUM_CLASSES_BOOK_COUNT\n",
        "global saved_model_weights\n",
        "global saved_model_all\n",
        "\n",
        "# Define paths to data\n",
        "# Your dir structure should look like this:\n",
        "#   project_root_dir/\n",
        "#   ├── images/\n",
        "#   │   ├── book1/\n",
        "#   │   │   ├── page1.jpg\n",
        "#   ...\n",
        "#   │   │   └── pageN.jpg\n",
        "#   ...\n",
        "#   │   └── bookN/\n",
        "#   │       ├── page1.jpg\n",
        "#   ...\n",
        "#   │       └── pageN.jpg\n",
        "#   └── annotations/\n",
        "#       ├── book1.xml\n",
        "#       ...\n",
        "#       └── bookN.xml\n",
        "#   ├── books.txt  <--------------------- this is the list of books in the dataset, i.e. `$ls images > books.txt`\n",
        "# dataset_root_dir = manga109_dir\n",
        "dataset_root_dir = os.path.join(project_root_dir, 'data/SmallTestData')\n",
        "train_images_dir = os.path.join(dataset_root_dir, 'images')\n",
        "train_annotations_dir = os.path.join(dataset_root_dir, 'annotations')\n",
        "test_images_dir = os.path.join(project_root_dir, 'data/ForTrainTesting/images')\n",
        "test_annotations_dir = os.path.join(\n",
        "    project_root_dir, 'data/ForTrainTesting/annotations')\n",
        "saved_model_weights = os.path.join(\n",
        "    tf_model_dir, 'manga109_ocr_model_weights.h5')\n",
        "saved_model_all = os.path.join(tf_model_dir, 'manga109_ocr_model.h5')\n",
        "\n",
        "# BEFORE you start the long training process, crash if the directories are not found\n",
        "if not os.path.exists(project_root_dir):\n",
        "    raise FileNotFoundError(\n",
        "        \"Project root directory not found at \" + project_root_dir)\n",
        "if not os.path.exists(dataset_root_dir):\n",
        "    raise FileNotFoundError(\n",
        "        \"Manga109 dataset directory not found at \" + dataset_root_dir)\n",
        "if not os.path.exists(tf_model_dir):\n",
        "    raise FileNotFoundError(\n",
        "        \"Target Output directory not found at \" + tf_model_dir)\n",
        "if not os.path.exists(train_images_dir):\n",
        "    raise FileNotFoundError(\n",
        "        \"Training images directory not found at \" + train_images_dir)\n",
        "if not os.path.exists(train_annotations_dir):\n",
        "    raise FileNotFoundError(\n",
        "        \"Training annotations directory not found at \" + train_annotations_dir)\n",
        "if not os.path.exists(test_images_dir):\n",
        "    raise FileNotFoundError(\n",
        "        \"Testing images directory not found at \" + test_images_dir)\n",
        "if not os.path.exists(test_annotations_dir):\n",
        "    raise FileNotFoundError(\n",
        "        \"Testing annotations directory not found at \" + test_annotations_dir)\n",
        "# check if books.txt exists, and whether the count matches the number of books in the images directory\n",
        "books_txt = os.path.join(dataset_root_dir, 'books.txt')\n",
        "if not os.path.exists(books_txt):\n",
        "    raise FileNotFoundError(\"Books.txt not found at \" + books_txt)\n",
        "# check if books.txt exists, and whether the count matches the number of books in the images directory\n",
        "books_count = len(os.listdir(train_images_dir))\n",
        "with open(books_txt, 'r') as f:\n",
        "    books_txt_count = len(f.readlines())\n",
        "if books_count != books_txt_count:\n",
        "    raise ValueError(\n",
        "        \"Mismatch in books count between images directory and books.txt\")\n",
        "print(\"Books count in images directory matches books.txt\")\n",
        "\n",
        "# Define constants\n",
        "IMAGE_SIZE = (224, 224)\n",
        "BATCH_SIZE = 32\n",
        "EPOCHS = 10\n",
        "LEARNING_RATE = 0.001\n",
        "#   Found 6519 images belonging to 83 classes.   <-- pay attention to class count here!\n",
        "#   Found 1587 images belonging to 83 classes.\n",
        "#   Epoch 1/10\n",
        "#     37/204 [====>.........................] - ETA: 28:49 - loss: 4.6653 - accuracy: 0.0220\n",
        "# update NUM_CLASSES_BOOK_COUNT based on number of books in annotations/images directory:\n",
        "# quickest is to use Manga109 API to count the books:\n",
        "dir_list = os.listdir(train_images_dir)\n",
        "print(\"Number of books found in the IMAGES dataset:\", len(dir_list))\n",
        "print(dir_list)\n",
        "dir_list = os.listdir(train_annotations_dir)\n",
        "print(\"Number of books found in the ANNOTATIONS dataset:\", len(dir_list))\n",
        "print(dir_list)\n",
        "manga109 = manga109api.Parser(root_dir=dataset_root_dir)\n",
        "# This is critical, if you get InvalidValue error, most likely, the class count has changed!\n",
        "NUM_CLASSES_BOOK_COUNT = len(manga109.books)\n",
        "print(\"Number of classes (books) found in the dataset:\", NUM_CLASSES_BOOK_COUNT)\n",
        "# make sure books_count == NUM_CLASSES_BOOK_COUNT\n",
        "if books_count != NUM_CLASSES_BOOK_COUNT:\n",
        "    raise ValueError(\n",
        "        \"Mismatch in books count between images directory and annotations directory\")\n",
        "print(\"# Books count in images directory matches annotations directory\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Training models...\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Import libraries\n",
        "import os\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers, models\n",
        "from tensorflow.keras.applications import MobileNetV2\n",
        "from tensorflow.keras.applications.mobilenet_v2 import preprocess_input\n",
        "from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "import numpy as np\n",
        "import keras\n",
        "\n",
        "# Define data generator, note that ImageDataGenerator (deprecated) is now in tf.keras.preprocessing.image\n",
        "# See https://www.tensorflow.org/api_docs/python/tf/keras/preprocessing/image_dataset_from_directory\n",
        "# datagen = tf.keras.preprocessing.image.ImageDataGenerator(\n",
        "#    preprocessing_function=preprocess_input,\n",
        "#    rescale=1. / 255,\n",
        "#    validation_split=0.2)\n",
        "# Usage: https://keras.io/api/data_loading/image/\n",
        "#   # Data directory structure:\n",
        "#   #   training_data/\n",
        "#   #   ...class_a/\n",
        "#   #   ......a_image_1.jpg\n",
        "#   #   ......a_image_2.jpg\n",
        "#   #   ...class_b/\n",
        "#   #   ......b_image_1.jpg\n",
        "#   #   ......b_image_2.jpg\n",
        "#   #   etc.\n",
        "#   train_ds = keras.utils.image_dataset_from_directory(\n",
        "#       directory='training_data/',\n",
        "#       labels='inferred',\n",
        "#       label_mode='categorical',\n",
        "#       batch_size=32,\n",
        "#       image_size=(256, 256))\n",
        "#   validation_ds = keras.utils.image_dataset_from_directory(\n",
        "#       directory='validation_data/',\n",
        "#       labels='inferred',\n",
        "#       label_mode='categorical',\n",
        "#       batch_size=32,\n",
        "#       image_size=(256, 256))\n",
        "#\n",
        "#   model = keras.applications.Xception(\n",
        "#       weights=None, input_shape=(256, 256, 3), classes=10)\n",
        "#   model.compile(optimizer='rmsprop', loss='categorical_crossentropy')\n",
        "#   model.fit(train_ds, epochs=10, validation_data=validation_ds)\n",
        "dataset = tf.keras.utils.image_dataset_from_directory(\n",
        "    train_images_dir,\n",
        "    labels='inferred',\n",
        "    label_mode='categorical',\n",
        "    class_names=None,\n",
        "    color_mode='rgb',\n",
        "    batch_size=BATCH_SIZE,\n",
        "    image_size=IMAGE_SIZE,\n",
        "    shuffle=True,\n",
        "    seed=123,\n",
        "    validation_split=0.2,\n",
        "    subset='training',\n",
        "    interpolation='bilinear',\n",
        "    follow_links=False\n",
        ")\n",
        "normalization_layer = tf.keras.layers.Rescaling(1./255)\n",
        "normalized_dataset = dataset.map(lambda x, y: (normalization_layer(x), y))\n",
        "\n",
        "\n",
        "# class_mode: {'input', 'categorical', 'sparse', 'binary', None}\n",
        "# train_generator = dataset.flow_from_directory(\n",
        "#    train_images_dir,\n",
        "#    target_size=IMAGE_SIZE,\n",
        "#    batch_size=BATCH_SIZE,\n",
        "#    class_mode='categorical',\n",
        "#    subset='training')\n",
        "train_generator = keras.utils.image_dataset_from_directory(\n",
        "    train_images_dir,\n",
        "    labels='inferred',\n",
        "    label_mode='categorical',\n",
        "    class_names=None,\n",
        "    color_mode='rgb',\n",
        "    batch_size=BATCH_SIZE,\n",
        "    image_size=IMAGE_SIZE,\n",
        "    shuffle=True,\n",
        "    seed=123,\n",
        "    validation_split=0.2,\n",
        "    subset='training',\n",
        "    interpolation='bilinear',\n",
        "    follow_links=False\n",
        ")\n",
        "\n",
        "# class_mode: {'input', 'categorical', 'sparse', 'binary', None}\n",
        "# val_generator = dataset.flow_from_directory(\n",
        "#    train_images_dir,\n",
        "#    target_size=IMAGE_SIZE,\n",
        "#    batch_size=BATCH_SIZE,\n",
        "#    class_mode='categorical',\n",
        "#    subset='validation')\n",
        "val_generator = keras.utils.image_dataset_from_directory(\n",
        "    train_images_dir,\n",
        "    labels='inferred',\n",
        "    label_mode='categorical',\n",
        "    class_names=None,\n",
        "    color_mode='rgb',\n",
        "    batch_size=BATCH_SIZE,\n",
        "    image_size=IMAGE_SIZE,\n",
        "    shuffle=True,\n",
        "    seed=123,\n",
        "    validation_split=0.2,\n",
        "    subset='validation',\n",
        "    interpolation='bilinear',\n",
        "    follow_links=False\n",
        ")\n",
        "\n",
        "my_model = keras.applications.Xception(\n",
        "    include_top=True,\n",
        "    weights='imagenet',\n",
        "    input_tensor=None,\n",
        "    input_shape=None,\n",
        "    pooling=None,\n",
        "    # classes=NUM_CLASSES_BOOK_COUNT,   # when using 'weights' as 'imagenet' with 'include_top' as true, 'classes' should be 1000\n",
        "    classifier_activation='softmax'\n",
        ")\n",
        "my_model.compile(optimizer='adam',\n",
        "                 loss='categorical_crossentropy',\n",
        "                 metrics=['accuracy'])\n",
        "my_model.summary()\n",
        "# my_model.fit(train_generator, validation_data=val_generator, epochs=EPOCHS)\n",
        "\n",
        "# Define SSD model\n",
        "\n",
        "\n",
        "def create_ssd_model(num_classes, image_size=(224, 224), weights='imagenet', include_top=False):\n",
        "    base_model = MobileNetV2(input_shape=(\n",
        "        image_size[0], image_size[1], 3), weights=weights, include_top=include_top)\n",
        "\n",
        "    for layer in base_model.layers:\n",
        "        layer.trainable = False\n",
        "\n",
        "    x = layers.GlobalAveragePooling2D()(base_model.output)\n",
        "    ssd_output = layers.Dense(num_classes, activation='softmax')(x)\n",
        "\n",
        "    model = models.Model(inputs=base_model.input, outputs=ssd_output)\n",
        "\n",
        "    return model\n",
        "\n",
        "\n",
        "# Compile model\n",
        "model = create_ssd_model(NUM_CLASSES_BOOK_COUNT, image_size=(\n",
        "    224, 224), weights='imagenet', include_top=False)\n",
        "optimizer = Adam(learning_rate=LEARNING_RATE)\n",
        "model.compile(optimizer=optimizer,\n",
        "              loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# Define callbacks for checkpoints and early stopping\n",
        "checkpoint_callback = ModelCheckpoint(filepath=tf_model_dir,\n",
        "                                      save_weights_only=True,\n",
        "                                      save_best_only=True,\n",
        "                                      monitor='val_loss',\n",
        "                                      mode='min',\n",
        "                                      verbose=1)\n",
        "early_stopping_callback = EarlyStopping(monitor='val_loss',\n",
        "                                        patience=3,\n",
        "                                        mode='min',\n",
        "                                        verbose=1)\n",
        "\n",
        "# Train model with callbacks\n",
        "# my_model.fit(train_generator, validation_data=val_generator, epochs=EPOCHS)\n",
        "history = model.fit(train_generator,\n",
        "                    epochs=EPOCHS,\n",
        "                    validation_data=val_generator,\n",
        "                    callbacks=[checkpoint_callback, early_stopping_callback])\n",
        "\n",
        "# save just the weights for my model\n",
        "model.save_weights(saved_model_weights)\n",
        "# saving with config and weights\n",
        "model.save(saved_model_all)\n",
        "\n",
        "# Evaluate model\n",
        "# https://www.tensorflow.org/api_docs/python/tf/keras/preprocessing/image/ImageDataGenerator\n",
        "# class_mode: {'input', 'categorical', 'sparse', 'binary', None}\n",
        "# test_generator = dataset.flow_from_directory(\n",
        "# test_images_dir,\n",
        "# target_size=IMAGE_SIZE,\n",
        "# batch_size=BATCH_SIZE,\n",
        "# class_mode='categorical',\n",
        "# )\n",
        "# test_generator = keras.utils.image_dataset_from_directory(\n",
        "#    test_images_dir,\n",
        "#    labels='inferred',\n",
        "#    label_mode='categorical',\n",
        "#    class_names=None,\n",
        "#    color_mode='rgb',\n",
        "#    batch_size=BATCH_SIZE,\n",
        "#    image_size=IMAGE_SIZE,\n",
        "#    shuffle=True,\n",
        "#    seed=123,\n",
        "#    validation_split=0.2,\n",
        "#    subset='validation',\n",
        "#    interpolation='bilinear',\n",
        "#    follow_links=False\n",
        "# )\n",
        "#\n",
        "# Evaluate model on test data\n",
        "# loss, accuracy = model.evaluate(test_generator)\n",
        "# print(\"Test Loss:\", loss)\n",
        "# print(\"Test Accuracy:\", accuracy)\n",
        "#"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Testing trained models:\n",
        "Load the models (weights) created on previous cell, and check/test it.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Import libraries\n",
        "import os\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers, models\n",
        "from tensorflow.keras.applications import MobileNetV2\n",
        "from tensorflow.keras.applications.mobilenet_v2 import preprocess_input\n",
        "from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "import numpy as np\n",
        "import keras\n",
        "\n",
        "if not os.path.exists(tf_model_dir):\n",
        "    raise FileNotFoundError(\n",
        "        \"Target Output directory not found at \" + tf_model_dir)\n",
        "if not os.path.exists(saved_model_all):\n",
        "    raise FileNotFoundError(\"Model file not found at \" + saved_model_all)\n",
        "if not os.path.exists(saved_model_weights):\n",
        "    raise FileNotFoundError(\n",
        "        \"Model weights file not found at \" + saved_model_weights)\n",
        "\n",
        "# load weights and config\n",
        "model = keras.models.load_model(saved_model_all)\n",
        "\n",
        "test_generator = keras.utils.image_dataset_from_directory(\n",
        "    test_images_dir,\n",
        "    labels='inferred',\n",
        "    label_mode='categorical',\n",
        "    class_names=None,\n",
        "    color_mode='rgb',\n",
        "    batch_size=BATCH_SIZE,\n",
        "    image_size=IMAGE_SIZE,\n",
        "    shuffle=True,\n",
        "    seed=123,\n",
        "    validation_split=0.2,\n",
        "    subset='validation',\n",
        "    interpolation='bilinear',\n",
        "    follow_links=False\n",
        ")\n",
        "\n",
        "# Evaluate model on test data\n",
        "loss, accuracy = model.evaluate(test_generator)\n",
        "print(\"Test Loss:\", loss)\n",
        "print(\"Test Accuracy:\", accuracy)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "authorship_tag": "ABX9TyMJa4AZntSaTgDIhORKwCOR",
      "include_colab_link": true,
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
