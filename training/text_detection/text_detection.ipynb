{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/HidekiAI/ML-manga109-OCR/blob/trunk/Untitled0.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "First, we want to make sure TensorFlow is installed in the Python (virtual) environment...\n",
        "\n",
        "-   TensorFlow Object Detection is now depracated\n",
        "-   TensorFlow Addons (for using TF-Vision) sunsets on May, 2024 and needs to be switched over to Keras, in which it should be accessible directly as long as TF is installed\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "S5BXth-cnCwW"
      },
      "outputs": [],
      "source": [
        "#!/bin/bash\n",
        "!pip install --upgrade pip\n",
        "\n",
        "!pip install -U --pre tensorflow==\"2.*\"\n",
        "!pip install tensorflow\n",
        "# Comment above and uncomment below if you want to install tensorflow-gpu instead of tensorflow on CoLab\n",
        "#!pip install tensorflow-gpu\n",
        "#pip install tensorflow[and-cuda]\n",
        "\n",
        "!pip install transformers\n",
        "!pip install tf-models-official\n",
        "!pip install tf-keras-vis"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Next, we'll need the (official) tools/libraries to read manga109 (annotation) data from https://github.com/manga109\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "!pip install manga109api"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "I want to know which version of TF is installed, I cannot run GPU version on my local machine...\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#!/usr/bin/env python\n",
        "import tensorflow as tf\n",
        "\n",
        "# Check TensorFlow version\n",
        "print(\"TensorFlow version:\", tf.__version__)\n",
        "\n",
        "# Check TensorFlow configuration\n",
        "print(\"TensorFlow configuration:\")\n",
        "print(tf.config.list_physical_devices('GPU'))  # List available GPUs\n",
        "print(tf.config.list_physical_devices('CPU'))  # List available CPUs"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Next, I'd like to absolutely make sure we have access to TF-Vision for text detection; Because tensorflow-addons has become sunset as of May, 2024, we just need to verify that keras is accessible...\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#!/usr/bin/env python\n",
        "from tensorflow.keras.layers import Dense, Conv2D, Flatten, MaxPooling2D\n",
        "from tensorflow.keras.models import Sequential\n",
        "import tensorflow as tf\n",
        "\n",
        "# Check TensorFlow version\n",
        "print(\"TensorFlow version:\", tf.__version__)\n",
        "\n",
        "# Access Keras functionality through tf.keras\n",
        "\n",
        "# Define a simple Sequential model\n",
        "model = Sequential([\n",
        "    Conv2D(16, 3, padding='same', activation='relu', input_shape=(32, 32, 3)),\n",
        "    MaxPooling2D(),\n",
        "    Conv2D(32, 3, padding='same', activation='relu'),\n",
        "    MaxPooling2D(),\n",
        "    Conv2D(64, 3, padding='same', activation='relu'),\n",
        "    MaxPooling2D(),\n",
        "    Flatten(),\n",
        "    Dense(128, activation='relu'),\n",
        "    Dense(10, activation='softmax')\n",
        "])\n",
        "\n",
        "# Compile the model\n",
        "model.compile(optimizer='adam',\n",
        "              loss='sparse_categorical_crossentropy',\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "# Print model summary\n",
        "model.summary()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Once TF-Vision is loaded, let's verify for sure via Python...\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#!/usr/bin/env python\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.applications import EfficientNetB0\n",
        "\n",
        "# Check TensorFlow version\n",
        "print(\"TensorFlow version:\", tf.__version__)\n",
        "\n",
        "# Try importing a TensorFlow Vision model (e.g., EfficientNet)\n",
        "try:\n",
        "    # Import the EfficientNetB0 model\n",
        "    model = EfficientNetB0(weights='imagenet')\n",
        "    print(\"TensorFlow Vision (via Keras) is accessible.\")\n",
        "except ImportError:\n",
        "    print(\"TensorFlow Vision (via Keras) is not accessible.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Note that below is ONLY necessary for Google CoLab to access your Google Drive. If on Notepad/Jupyter, do the following instead (not exact, just the example):\n",
        "\n",
        "-   Linux: make sure to `ln -sv ~/Google/MyDrive /content/drive` to softlink your Google G-Drive as `/content/drive`\n",
        "-   Windows: From DOS Command Prompt (right clock to launch as Admin) `mklink.exe /D \"C:/content/drive\" \"C:/Users/HidekiAI/Google/MyDrive/\"` to create a dir-junction\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#!/usr/bin/python\n",
        "# No need to execute this if running locally, this is only for Google CoLab usage\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Verify either via BASH or python that we can access `/content/drive` mount\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#!/bin/bash\n",
        "! pwd && [ -e /content/drive/MyDrive ] || echo \"Unable to validate Google Drive from bash script\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#!/usr/bin/env python\n",
        "import os\n",
        "\n",
        "# directory path to the Manga109 dataset (read-only)\n",
        "global manga109_dir\n",
        "# directory path to the TensorFlow TFRecord model (read-write)\n",
        "global tf_model_dir\n",
        "\n",
        "# Check if Google Drive is mounted and/or locally have symlink (or junctions) to access '/content/drive/MyDrive'\n",
        "if os.path.isdir('/content/drive'):\n",
        "    # list contents of the root directory of Google drive\n",
        "    # change this to your own path\n",
        "    root_paths = '/content/drive/MyDrive/projects/ML-manga-ocr-rust/'\n",
        "    data_paths = os.path.join(root_paths, 'data/')  # should pre-exist!\n",
        "    tf_model_dir = os.path.join(data_paths, 'tf_model/')\n",
        "    # mkdir if not exists\n",
        "    if not os.path.exists(tf_model_dir):\n",
        "        os.makedirs(tf_model_dir)\n",
        "        print('Created TensorFlow model directory at ', tf_model_dir)\n",
        "\n",
        "    drive_files = os.listdir(root_paths)\n",
        "    print(drive_files)\n",
        "    drive_files = os.listdir(data_paths)\n",
        "    print(drive_files)\n",
        "    zip_path = os.path.join(root_paths, 'data/Manga109s.zip')\n",
        "    if os.path.exists(zip_path):\n",
        "        # only UNZIP IF dir does not exist, else assume it's already unzipped\n",
        "        if not os.path.exists(data_paths):\n",
        "            # os.makedirs(data_paths)\n",
        "            #!unzip '{zip_path}' -d '{data_paths}'\n",
        "            with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
        "                zip_ref.extractall(data_paths)\n",
        "                print('Unzipped the data to ', data_paths)\n",
        "    drive_files = os.listdir(data_paths)\n",
        "    manga109_dir = os.path.join(\n",
        "        data_paths, 'Manga109s/Manga109s_released_2023_12_07/')\n",
        "    data_dir_files = os.listdir(manga109_dir)\n",
        "    print(data_dir_files)\n",
        "    # lastly, notify users of their license by printing the readme.txt\n",
        "    readme_path = os.path.join(manga109_dir, 'readme.txt')\n",
        "    with open(readme_path, 'r', encoding=\"utf-8\") as file:\n",
        "        print(file.read())\n",
        "else:\n",
        "    print(\"Google Drive is not mounted.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Now that we have manga dir accessible, let's try out the manga109api...\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#!/usr/bin/env python\n",
        "import manga109api\n",
        "from PIL import Image, ImageDraw\n",
        "\n",
        "\n",
        "def draw_rectangle(img, x0, y0, x1, y1, annotation_type):\n",
        "    assert annotation_type in [\"body\", \"face\", \"frame\", \"text\"]\n",
        "    color = {\"body\": \"#258039\", \"face\": \"#f5be41\",\n",
        "             \"frame\": \"#31a9b8\", \"text\": \"#cf3721\"}[annotation_type]\n",
        "    draw = ImageDraw.Draw(img)\n",
        "    draw.rectangle([x0, y0, x1, y1], outline=color, width=10)\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    book = \"YumeiroCooking\"\n",
        "    page_index = 6\n",
        "\n",
        "    p = manga109api.Parser(root_dir=manga109_dir)\n",
        "    annotation = p.get_annotation(book=book)\n",
        "    img = Image.open(p.img_path(book=book, index=page_index))\n",
        "\n",
        "    for annotation_type in [\"body\", \"face\", \"frame\", \"text\"]:\n",
        "        rois = annotation[\"page\"][page_index][annotation_type]\n",
        "        for roi in rois:\n",
        "            draw_rectangle(img, roi[\"@xmin\"], roi[\"@ymin\"],\n",
        "                           roi[\"@xmax\"], roi[\"@ymax\"], annotation_type)\n",
        "\n",
        "    # Display preprocessed image\n",
        "    import matplotlib.pyplot as plt\n",
        "    plt.imshow(img)\n",
        "    plt.axis('off')\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Load and Preprocess Images with TensorFlow:\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "If you did see an image load up with rectangles around texts, you are now ready to integrate it with TF-Vision...\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#!/usr/bin/env python\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import tensorflow as tf\n",
        "import manga109api\n",
        "from PIL import Image, ImageDraw\n",
        "\n",
        "# Initialize Manga109 API\n",
        "manga109 = manga109api.Parser(root_dir=manga109_dir)\n",
        "\n",
        "# Choose a manga volume and page index\n",
        "volume = 'YumeiroCooking'\n",
        "page_index = 6\n",
        "\n",
        "# Load image using Manga109 API\n",
        "image = Image.open(manga109.img_path(book=volume, index=page_index))\n",
        "\n",
        "# Preprocess image using TensorFlow Keras\n",
        "image = tf.keras.preprocessing.image.img_to_array(image)\n",
        "image = tf.keras.applications.efficientnet.preprocess_input(image)\n",
        "\n",
        "# Display preprocessed image\n",
        "plt.imshow(image)\n",
        "plt.axis('off')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "If the above worked for single book/volume, we can now iterate the ENTIRE books it knows about:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#!/usr/bin/env python\n",
        "\n",
        "import tensorflow as tf\n",
        "import manga109api\n",
        "\n",
        "global books_preprocessed\n",
        "\n",
        "# Initialize Manga109 API\n",
        "manga109 = manga109api.Parser(root_dir=manga109_dir)\n",
        "\n",
        "# structure of KVP:\n",
        "#    'YumeiroCooking/000.jpg': [{'class': 'text', 'xmin': 100, 'ymin': 50, 'xmax': 200, 'ymax': 150}],\n",
        "books_preprocessed = []\n",
        "\n",
        "# Iterate through all books\n",
        "for book in manga109.books:\n",
        "    print(f\"{book}\", end=\",\")\n",
        "    annotations_of_this_book = manga109.get_annotation(book)\n",
        "    pages = annotations_of_this_book['page']\n",
        "\n",
        "    # Iterate through all pages in the book\n",
        "    # sample output:\n",
        "    #   Page= 0 , page= {'@index': 0, '@width': 1654, '@height': 1170, 'frame': [], 'face': [], 'body': [], 'text': []}\n",
        "    #   Page= 2 , page= {'@index': 2, '@width': 1654, '@height': 1170, 'frame': [{'@id': '00000000', '@xmin': 83, '@ymin': 86, '@xmax': 751, '@ymax': 1090, 'type': 'frame'}], 'face': [{'@id': '00000004', '@xmin': 406, '@ymin': 684, '@xmax': 456, '@ymax': 764, '@character': '00000003', 'type': 'face'}], 'body': [{'@id': '00000002', '@xmin': 178, '@ymin': 660, '@xmax': 548, '@ymax': 965, '@character': '00000003', 'type': 'body'}], 'text': [{'@id': '00000001', '@xmin': 550, '@ymin': 660, '@xmax': 583, '@ymax': 696, '#text': 'あ', 'type': 'text'}]}\n",
        "    #   Page= 7 , page= {'@index': 7, '@width': 1654, '@height': 1170, 'frame': [{'@id': '0000007e', '@xmin': 53, '@ymin': 6, '@xmax': 419, '@ymax': 361, 'type': 'frame'}, {'@id': '00000086', '@xmin': 901, '@ymin': 93, '@xmax': 1382, '@ymax': 500, 'type': 'frame'}, {'@id': '00000088', '@xmin': 901, '@ymin': 519, '@xmax': 1567, '@ymax': 1169, 'type': 'frame'}, {'@id': '0000008c', '@xmin': 435, '@ymin': 98, '@xmax': 747, '@ymax': 361, 'type': 'frame'}, {'@id': '0000008d', '@xmin': 5, '@ymin': 361, '@xmax': 819, '@ymax': 1169, 'type': 'frame'}, {'@id': '0000008e', '@xmin': 1385, '@ymin': 96, '@xmax': 1565, '@ymax': 501, 'type': 'frame'}], 'face': [{'@id': '0000007a', '@xmin': 223, '@ymin': 199, '@xmax': 348, '@ymax': 286, '@character': '00000003', 'type': 'face'}, {'@id': '0000007f', '@xmin': 1117, '@ymin': 204, '@xmax': 1231, '@ymax': 294, '@character': '00000003', 'type': 'face'}, {'@id': '00000080', '@xmin': 1403, '@ymin': 449, '@xmax': 1454, '@ymax': 494, '@character': '00000010', 'type': 'face'}, {'@id': '00000083', '@xmin': 492, '@ymin': 276, '@xmax': 541, '@ymax': 316, '@character': '00000010', 'type': 'face'}], 'body': [{'@id': '00000077', '@xmin': 431, '@ymin': 249, '@xmax': 597, '@ymax': 363, '@character': '00000010', 'type': 'body'}, {'@id': '00000079', '@xmin': 1400, '@ymin': 444, '@xmax': 1458, '@ymax': 501, '@character': '00000010', 'type': 'body'}, {'@id': '00000081', '@xmin': 161, '@ymin': 91, '@xmax': 419, '@ymax': 363, '@character': '00000003', 'type': 'body'}, {'@id': '00000087', '@xmin': 1043, '@ymin': 114, '@xmax': 1364, '@ymax': 501, '@character': '00000003', 'type': 'body'}, {'@id': '0000008f', '@xmin': 37, '@ymin': 415, '@xmax': 766, '@ymax': 1012, '@character': '00000090', 'type': 'body'}], 'text': [{'@id': '00000078', '@xmin': 463, '@ymin': 695, '@xmax': 477, '@ymax': 736, '#text': 'しょうぶ.....', 'type': 'text'}, {'@id': '0000007b', '@xmin': 217, '@ymin': 348, '@xmax': 268, '@ymax': 456, '#text': 'こらっ\\nこのやろ', 'type': 'text'}, {'@id': '0000007c', '@xmin': 55, '@ymin': 251, '@xmax': 95, '@ymax': 334, '#text': 'おいっ', 'type': 'text'}, {'@id': '0000007d', '@xmin': 693, '@ymin': 92, '@xmax': 749, '@ymax': 178, '#text': '出てこいっ！', 'type': 'text'}, {'@id': '00000082', '@xmin': 1284, '@ymin': 78, '@xmax': 1380, '@ymax': 300, '#text': 'そこかっ！', 'type': 'text'}, {'@id': '00000084', '@xmin': 573, '@ymin': 260, '@xmax': 622, '@ymax': 316, '#text': 'むちゃ言うな', 'type': 'text'}, {'@id': '00000085', '@xmin': 397, '@ymin': 90, '@xmax': 414, '@ymax': 173, '#text': 'どこだ！', 'type': 'text'}, {'@id': '00000089', '@xmin': 327, '@ymin': 723, '@xmax': 374, '@ymax': 772, '#text': 'なんちゃって\\nはは....', 'type': 'text'}, {'@id': '0000008a', '@xmin': 532, '@ymin': 483, '@xmax': 631, '@ymax': 660, '#text': 'おいっ......てばっ\\n出てきてわたしと', 'type': 'text'}, {'@id': '0000008b', '@xmin': 89, '@ymin': 85, '@xmax': 175, '@ymax': 203, '#text': '出て来て私と勝負しろっ！', 'type': 'text'}]}\n",
        "    #   Page= 10 , page= {'@index': 10, '@width': 1654, '@height': 1170, 'frame': [{'@id': '000000cd', '@xmin': 341, '@ymin': 96, '@xmax': 485, '@ymax': 354, 'type': 'frame'}, {'@id': '000000cf', '@xmin': 834, '@ymin': 505, '@xmax': 1648, '@ymax': 745, 'type': 'frame'}, {'@id': '000000d3', '@xmin': 897, '@ymin': 750, '@xmax': 1216, '@ymax': 1169, 'type': 'frame'}, {'@id': '000000d5', '@xmin': 80, '@ymin': 721, '@xmax': 748, '@ymax': 1098, 'type': 'frame'}, {'@id': '000000d6', '@xmin': 1098, '@ymin': 1, '@xmax': 1653, '@ymax': 502, 'type': 'frame'}, {'@id': '000000da', '@xmin': 1214, '@ymin': 746, '@xmax': 1565, '@ymax': 1096, 'type': 'frame'}, {'@id': '000000e1', '@xmin': 489, '@ymin': 3, '@xmax': 745, '@ymax': 360, 'type': 'frame'}, {'@id': '000000e2', '@xmin': 82, '@ymin': 96, '@xmax': 340, '@ymax': 356, 'type': 'frame'}, {'@id': '000000e3', '@xmin': 1, '@ymin': 372, '@xmax': 817, '@ymax': 720, 'type': 'frame'}, {'@id': '000000e4', '@xmin': 901, '@ymin': 100, '@xmax': 1092, '@ymax': 502, 'type': 'frame'}], 'face': [{'@id': '000000d0', '@xmin': 987, '@ymin': 163, '@xmax': 1029, '@ymax': 192, '@character': '00000010', 'type': 'face'}, {'@id': '000000d4', '@xmin': 350, '@ymin': 150, '@xmax': 471, '@ymax': 242, '@character': '00000003', 'type': 'face'}, {'@id': '000000d9', '@xmin': 1043, '@ymin': 775, '@xmax': 1088, '@ymax': 808, '@character': '00000010', 'type': 'face'}], 'body': [{'@id': '000000ce', '@xmin': 595, '@ymin': 129, '@xmax': 662, '@ymax': 227, '@character': '00000010', 'type': 'body'}, {'@id': '000000d1', '@xmin': 343, '@ymin': 100, '@xmax': 480, '@ymax': 351, '@character': '00000003', 'type': 'body'}, {'@id': '000000d7', '@xmin': 969, '@ymin': 138, '@xmax': 1050, '@ymax': 367, '@character': '00000010', 'type': 'body'}, {'@id': '000000d8', '@xmin': 991, '@ymin': 752, '@xmax': 1148, '@ymax': 1062, '@character': '00000010', 'type': 'body'}, {'@id': '000000db', '@xmin': 1310, '@ymin': 515, '@xmax': 1623, '@ymax': 743, '@character': '00000090', 'type': 'body'}, {'@id': '000000dd', '@xmin': 139, '@ymin': 385, '@xmax': 817, '@ymax': 723, '@character': '00000090', 'type': 'body'}, {'@id': '000000df', '@xmin': 383, '@ymin': 372, '@xmax': 477, '@ymax': 459, '@character': '00000010', 'type': 'body'}], 'text': [{'@id': '000000d2', '@xmin': 698, '@ymin': 238, '@xmax': 711, '@ymax': 284, '#text': 'ブン？', 'type': 'text'}, {'@id': '000000dc', '@xmin': 356, '@ymin': 273, '@xmax': 403, '@ymax': 340, '#text': 'あの人.....', 'type': 'text'}, {'@id': '000000de', '@xmin': 1131, '@ymin': 752, '@xmax': 1200, '@ymax': 841, '#text': 'わっ', 'type': 'text'}, {'@id': '000000e0', '@xmin': 482, '@ymin': 91, '@xmax': 498, '@ymax': 145, '#text': 'あれ？', 'type': 'text'}]}\n",
        "    # NOTE: each page can have multiple text regions\n",
        "    for page_index, page in enumerate(pages):\n",
        "        # print(\"\\tPage=\", page_index, \", page=\", page)\n",
        "\n",
        "        # Load image using Manga109 API\n",
        "        image = Image.open(manga109.img_path(book=volume, index=page_index))\n",
        "\n",
        "        # Display preprocessed image (optional)\n",
        "        # import matplotlib.pyplot as plt\n",
        "        # plt.imshow(image)\n",
        "        # plt.axis('off')\n",
        "        # plt.show()\n",
        "\n",
        "        frame_rects = page.get('frame')\n",
        "        face_rects = page.get('face')\n",
        "        body_rects = page.get('body')\n",
        "        text_rects = page.get('text')\n",
        "\n",
        "        # for debugging, if we find pages with texts, print them\n",
        "        #if text_rects.__len__() > 0:\n",
        "        #    print(page_index, end=\" \")\n",
        "\n",
        "        # Preprocess image using TensorFlow Keras\n",
        "        image = tf.keras.preprocessing.image.img_to_array(image)\n",
        "        image = tf.keras.applications.efficientnet.preprocess_input(image)\n",
        "\n",
        "        books_preprocessed.append(image)\n",
        "\n",
        "    #print()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Now that we have a preprocessed image that are all of same dimensions, we'll create TFRecords for saving what is to be trained for future.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#!/usr/bin/env python\n",
        "import tensorflow as tf\n",
        "\n",
        "\n",
        "def create_tf_manga109_rects_from_page(fully_qualified_image_path, page):\n",
        "    # change dir to manga109_dir\n",
        "\n",
        "    with tf.io.gfile.GFile(fully_qualified_image_path, 'rb') as f:\n",
        "        encoded_image = f.read()\n",
        "\n",
        "    image = tf.image.decode_jpeg(encoded_image, channels=3)\n",
        "    height = image.shape[0]\n",
        "    width = image.shape[1]\n",
        "\n",
        "    filename = os.path.basename(fully_qualified_image_path)\n",
        "\n",
        "    image_format = b'jpeg'\n",
        "    xmins = []\n",
        "    xmaxs = []\n",
        "    ymins = []\n",
        "    ymaxs = []\n",
        "    classes_text = []\n",
        "    classes = []\n",
        "\n",
        "    for annotation in annotations:\n",
        "        xmin = annotation['xmin'] / width\n",
        "        xmax = annotation['xmax'] / width\n",
        "        ymin = annotation['ymin'] / height\n",
        "        ymax = annotation['ymax'] / height\n",
        "\n",
        "        xmins.append(xmin)\n",
        "        xmaxs.append(xmax)\n",
        "        ymins.append(ymin)\n",
        "        ymaxs.append(ymax)\n",
        "\n",
        "        class_text = annotation['class'].encode('utf8')\n",
        "        classes_text.append(class_text)\n",
        "        # classes.append(label_map[class_text])\n",
        "\n",
        "    tf_class_rects = tf.train.XXXXXX(features=tf.train.Features(feature={\n",
        "        'image/height': tf.train.Feature(int64_list=tf.train.Int64List(value=[height])),\n",
        "        'image/width': tf.train.Feature(int64_list=tf.train.Int64List(value=[width])),\n",
        "        'image/filename': tf.train.Feature(bytes_list=tf.train.BytesList(value=[filename.encode('utf8')])),\n",
        "        'image/source_id': tf.train.Feature(bytes_list=tf.train.BytesList(value=[filename.encode('utf8')])),\n",
        "        'image/encoded': tf.train.Feature(bytes_list=tf.train.BytesList(value=[encoded_image])),\n",
        "        'image/format': tf.train.Feature(bytes_list=tf.train.BytesList(value=[image_format])),\n",
        "        'image/object/bbox/xmin': tf.train.Feature(float_list=tf.train.FloatList(value=xmins)),\n",
        "        'image/object/bbox/xmax': tf.train.Feature(float_list=tf.train.FloatList(value=xmaxs)),\n",
        "        'image/object/bbox/ymin': tf.train.Feature(float_list=tf.train.FloatList(value=ymins)),\n",
        "        'image/object/bbox/ymax': tf.train.Feature(float_list=tf.train.FloatList(value=ymaxs)),\n",
        "        'image/object/class/text': tf.train.Feature(bytes_list=tf.train.BytesList(value=classes_text)),\n",
        "        'image/object/class/label': tf.train.Feature(int64_list=tf.train.Int64List(value=classes)),\n",
        "    }))\n",
        "\n",
        "    return tf_class_rects\n",
        "\n",
        "\n",
        "def create_tf_records(annotation_data, output_path):\n",
        "    writer = tf.io.TFRecordWriter(output_path)\n",
        "\n",
        "    for image_path, annotations in annotation_data.items():\n",
        "        fully_qualified_image_path = os.path.join(\n",
        "            manga109_dir, 'image', image_path)\n",
        "        tf_result = create_tf_manga109_rects(\n",
        "            fully_qualified_image_path, annotations)\n",
        "        writer.write(tf_result.SerializeToString())\n",
        "\n",
        "    writer.close()\n",
        "\n",
        "# Example usage:\n",
        "# annotation_data = {\n",
        "#    'YumeiroCooking/000.jpg': [{'class': 'body', 'xmin': 100, 'ymin': 50, 'xmax': 200, 'ymax': 150}],\n",
        "#    'YumeiroCooking/001.jpg': [{'class': 'face', 'xmin': 50, 'ymin': 30, 'xmax': 100, 'ymax': 80}],\n",
        "# }\n",
        "# Iterate through all books\n",
        "for book in manga109.books:\n",
        "    print(f\"Processing book: {book}\")\n",
        "    annotations_of_this_book = manga109.get_annotation(book)\n",
        "    pages = annotations_of_this_book['page']\n",
        "\n",
        "    # Iterate through all pages in the book\n",
        "    # sample output:\n",
        "    #   Page= 0 , page= {'@index': 0, '@width': 1654, '@height': 1170, 'frame': [], 'face': [], 'body': [], 'text': []}\n",
        "    #   Page= 2 , page= {'@index': 2, '@width': 1654, '@height': 1170, 'frame': [{'@id': '00000000', '@xmin': 83, '@ymin': 86, '@xmax': 751, '@ymax': 1090, 'type': 'frame'}], 'face': [{'@id': '00000004', '@xmin': 406, '@ymin': 684, '@xmax': 456, '@ymax': 764, '@character': '00000003', 'type': 'face'}], 'body': [{'@id': '00000002', '@xmin': 178, '@ymin': 660, '@xmax': 548, '@ymax': 965, '@character': '00000003', 'type': 'body'}], 'text': [{'@id': '00000001', '@xmin': 550, '@ymin': 660, '@xmax': 583, '@ymax': 696, '#text': 'あ', 'type': 'text'}]}\n",
        "    #   Page= 7 , page= {'@index': 7, '@width': 1654, '@height': 1170, 'frame': [{'@id': '0000007e', '@xmin': 53, '@ymin': 6, '@xmax': 419, '@ymax': 361, 'type': 'frame'}, {'@id': '00000086', '@xmin': 901, '@ymin': 93, '@xmax': 1382, '@ymax': 500, 'type': 'frame'}, {'@id': '00000088', '@xmin': 901, '@ymin': 519, '@xmax': 1567, '@ymax': 1169, 'type': 'frame'}, {'@id': '0000008c', '@xmin': 435, '@ymin': 98, '@xmax': 747, '@ymax': 361, 'type': 'frame'}, {'@id': '0000008d', '@xmin': 5, '@ymin': 361, '@xmax': 819, '@ymax': 1169, 'type': 'frame'}, {'@id': '0000008e', '@xmin': 1385, '@ymin': 96, '@xmax': 1565, '@ymax': 501, 'type': 'frame'}], 'face': [{'@id': '0000007a', '@xmin': 223, '@ymin': 199, '@xmax': 348, '@ymax': 286, '@character': '00000003', 'type': 'face'}, {'@id': '0000007f', '@xmin': 1117, '@ymin': 204, '@xmax': 1231, '@ymax': 294, '@character': '00000003', 'type': 'face'}, {'@id': '00000080', '@xmin': 1403, '@ymin': 449, '@xmax': 1454, '@ymax': 494, '@character': '00000010', 'type': 'face'}, {'@id': '00000083', '@xmin': 492, '@ymin': 276, '@xmax': 541, '@ymax': 316, '@character': '00000010', 'type': 'face'}], 'body': [{'@id': '00000077', '@xmin': 431, '@ymin': 249, '@xmax': 597, '@ymax': 363, '@character': '00000010', 'type': 'body'}, {'@id': '00000079', '@xmin': 1400, '@ymin': 444, '@xmax': 1458, '@ymax': 501, '@character': '00000010', 'type': 'body'}, {'@id': '00000081', '@xmin': 161, '@ymin': 91, '@xmax': 419, '@ymax': 363, '@character': '00000003', 'type': 'body'}, {'@id': '00000087', '@xmin': 1043, '@ymin': 114, '@xmax': 1364, '@ymax': 501, '@character': '00000003', 'type': 'body'}, {'@id': '0000008f', '@xmin': 37, '@ymin': 415, '@xmax': 766, '@ymax': 1012, '@character': '00000090', 'type': 'body'}], 'text': [{'@id': '00000078', '@xmin': 463, '@ymin': 695, '@xmax': 477, '@ymax': 736, '#text': 'しょうぶ.....', 'type': 'text'}, {'@id': '0000007b', '@xmin': 217, '@ymin': 348, '@xmax': 268, '@ymax': 456, '#text': 'こらっ\\nこのやろ', 'type': 'text'}, {'@id': '0000007c', '@xmin': 55, '@ymin': 251, '@xmax': 95, '@ymax': 334, '#text': 'おいっ', 'type': 'text'}, {'@id': '0000007d', '@xmin': 693, '@ymin': 92, '@xmax': 749, '@ymax': 178, '#text': '出てこいっ！', 'type': 'text'}, {'@id': '00000082', '@xmin': 1284, '@ymin': 78, '@xmax': 1380, '@ymax': 300, '#text': 'そこかっ！', 'type': 'text'}, {'@id': '00000084', '@xmin': 573, '@ymin': 260, '@xmax': 622, '@ymax': 316, '#text': 'むちゃ言うな', 'type': 'text'}, {'@id': '00000085', '@xmin': 397, '@ymin': 90, '@xmax': 414, '@ymax': 173, '#text': 'どこだ！', 'type': 'text'}, {'@id': '00000089', '@xmin': 327, '@ymin': 723, '@xmax': 374, '@ymax': 772, '#text': 'なんちゃって\\nはは....', 'type': 'text'}, {'@id': '0000008a', '@xmin': 532, '@ymin': 483, '@xmax': 631, '@ymax': 660, '#text': 'おいっ......てばっ\\n出てきてわたしと', 'type': 'text'}, {'@id': '0000008b', '@xmin': 89, '@ymin': 85, '@xmax': 175, '@ymax': 203, '#text': '出て来て私と勝負しろっ！', 'type': 'text'}]}\n",
        "    #   Page= 10 , page= {'@index': 10, '@width': 1654, '@height': 1170, 'frame': [{'@id': '000000cd', '@xmin': 341, '@ymin': 96, '@xmax': 485, '@ymax': 354, 'type': 'frame'}, {'@id': '000000cf', '@xmin': 834, '@ymin': 505, '@xmax': 1648, '@ymax': 745, 'type': 'frame'}, {'@id': '000000d3', '@xmin': 897, '@ymin': 750, '@xmax': 1216, '@ymax': 1169, 'type': 'frame'}, {'@id': '000000d5', '@xmin': 80, '@ymin': 721, '@xmax': 748, '@ymax': 1098, 'type': 'frame'}, {'@id': '000000d6', '@xmin': 1098, '@ymin': 1, '@xmax': 1653, '@ymax': 502, 'type': 'frame'}, {'@id': '000000da', '@xmin': 1214, '@ymin': 746, '@xmax': 1565, '@ymax': 1096, 'type': 'frame'}, {'@id': '000000e1', '@xmin': 489, '@ymin': 3, '@xmax': 745, '@ymax': 360, 'type': 'frame'}, {'@id': '000000e2', '@xmin': 82, '@ymin': 96, '@xmax': 340, '@ymax': 356, 'type': 'frame'}, {'@id': '000000e3', '@xmin': 1, '@ymin': 372, '@xmax': 817, '@ymax': 720, 'type': 'frame'}, {'@id': '000000e4', '@xmin': 901, '@ymin': 100, '@xmax': 1092, '@ymax': 502, 'type': 'frame'}], 'face': [{'@id': '000000d0', '@xmin': 987, '@ymin': 163, '@xmax': 1029, '@ymax': 192, '@character': '00000010', 'type': 'face'}, {'@id': '000000d4', '@xmin': 350, '@ymin': 150, '@xmax': 471, '@ymax': 242, '@character': '00000003', 'type': 'face'}, {'@id': '000000d9', '@xmin': 1043, '@ymin': 775, '@xmax': 1088, '@ymax': 808, '@character': '00000010', 'type': 'face'}], 'body': [{'@id': '000000ce', '@xmin': 595, '@ymin': 129, '@xmax': 662, '@ymax': 227, '@character': '00000010', 'type': 'body'}, {'@id': '000000d1', '@xmin': 343, '@ymin': 100, '@xmax': 480, '@ymax': 351, '@character': '00000003', 'type': 'body'}, {'@id': '000000d7', '@xmin': 969, '@ymin': 138, '@xmax': 1050, '@ymax': 367, '@character': '00000010', 'type': 'body'}, {'@id': '000000d8', '@xmin': 991, '@ymin': 752, '@xmax': 1148, '@ymax': 1062, '@character': '00000010', 'type': 'body'}, {'@id': '000000db', '@xmin': 1310, '@ymin': 515, '@xmax': 1623, '@ymax': 743, '@character': '00000090', 'type': 'body'}, {'@id': '000000dd', '@xmin': 139, '@ymin': 385, '@xmax': 817, '@ymax': 723, '@character': '00000090', 'type': 'body'}, {'@id': '000000df', '@xmin': 383, '@ymin': 372, '@xmax': 477, '@ymax': 459, '@character': '00000010', 'type': 'body'}], 'text': [{'@id': '000000d2', '@xmin': 698, '@ymin': 238, '@xmax': 711, '@ymax': 284, '#text': 'ブン？', 'type': 'text'}, {'@id': '000000dc', '@xmin': 356, '@ymin': 273, '@xmax': 403, '@ymax': 340, '#text': 'あの人.....', 'type': 'text'}, {'@id': '000000de', '@xmin': 1131, '@ymin': 752, '@xmax': 1200, '@ymax': 841, '#text': 'わっ', 'type': 'text'}, {'@id': '000000e0', '@xmin': 482, '@ymin': 91, '@xmax': 498, '@ymax': 145, '#text': 'あれ？', 'type': 'text'}]}\n",
        "    # NOTE: each page can have multiple text regions\n",
        "    for page_index, page in enumerate(pages):\n",
        "        # print(\"\\tPage=\", page_index, \", page=\", page)\n",
        "\n",
        "        # Load image using Manga109 API\n",
        "        image = Image.open(manga109.img_path(book=volume, index=page_index))\n",
        "\n",
        "        # Display preprocessed image (optional)\n",
        "        # import matplotlib.pyplot as plt\n",
        "        # plt.imshow(image)\n",
        "        # plt.axis('off')\n",
        "        # plt.show()\n",
        "\n",
        "        frame_rects = page.get('frame')\n",
        "        face_rects = page.get('face')\n",
        "        body_rects = page.get('body')\n",
        "        text_rects = page.get('text')\n",
        "\n",
        "        # for debugging, if we find pages with texts, print them\n",
        "        if text_rects.__len__() > 0:\n",
        "            print(page_index, end=\" \")\n",
        "\n",
        "\n",
        "        output_path = os.path.join(tf_model_dir, 'manga109_detection.tfrecords')\n",
        "        create_tf_records(books_preprocessed, output_path)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Model and Loss Function:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#!/usr/bin/env python\n",
        "from tensorflow.keras.layers import Input, Conv2D, MaxPooling2D, Flatten, Dense\n",
        "from tensorflow.keras.models import Model\n",
        "\n",
        "# Define your model architecture\n",
        "\n",
        "\n",
        "def create_model(input_shape, num_classes):\n",
        "    inputs = Input(shape=input_shape)\n",
        "    x = Conv2D(32, (3, 3), activation='relu')(inputs)\n",
        "    x = MaxPooling2D((2, 2))(x)\n",
        "    x = Conv2D(64, (3, 3), activation='relu')(x)\n",
        "    x = MaxPooling2D((2, 2))(x)\n",
        "    x = Flatten()(x)\n",
        "    x = Dense(128, activation='relu')(x)\n",
        "    outputs = Dense(num_classes, activation='softmax')(x)\n",
        "\n",
        "    model = Model(inputs=inputs, outputs=outputs)\n",
        "    return model\n",
        "\n",
        "\n",
        "# Compile the model\n",
        "model = create_model(input_shape=(224, 224, 3), num_classes=4)\n",
        "model.compile(optimizer='adam', loss='categorical_crossentropy',\n",
        "              metrics=['accuracy'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Data Augmentation:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#!/usr/bin/env python\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "\n",
        "# Define data augmentation parameters\n",
        "datagen = ImageDataGenerator(\n",
        "    rotation_range=10,\n",
        "    width_shift_range=0.1,\n",
        "    height_shift_range=0.1,\n",
        "    zoom_range=0.1,\n",
        "    horizontal_flip=True,\n",
        "    vertical_flip=True,\n",
        "    fill_mode='nearest'\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Training...\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#!/usr/bin/env python\n",
        "# Train the model\n",
        "batch_size = 32\n",
        "epochs = 10\n",
        "\n",
        "# Convert annotations to one-hot encoding for classes\n",
        "# Assuming you have a function to convert class labels to numeric IDs\n",
        "# For simplicity, let's assume classes are already numeric IDs\n",
        "# If not, you need to convert them using label encoding\n",
        "# Also, you need to modify your create_tf_example function to include numeric class labels\n",
        "# Assuming you have label_map as a dictionary mapping class labels to numeric IDs\n",
        "# label_map = {'body': 0, 'face': 1, 'frame': 2, 'text': 3}\n",
        "\n",
        "# Define steps per epoch based on the number of training samples and batch size\n",
        "steps_per_epoch = len(annotation_data) // batch_size\n",
        "\n",
        "# Train the model using fit_generator\n",
        "model.fit_generator(\n",
        "    datagen.flow_from_directory(\n",
        "        '/path/to/dataset', target_size=(224, 224), batch_size=batch_size),\n",
        "    steps_per_epoch=steps_per_epoch,\n",
        "    epochs=epochs\n",
        ")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "authorship_tag": "ABX9TyMJa4AZntSaTgDIhORKwCOR",
      "include_colab_link": true,
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
